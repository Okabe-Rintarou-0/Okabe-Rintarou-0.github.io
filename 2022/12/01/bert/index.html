
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <title>BERT 笔记 | Okabe&#39;s LAB</title>
    <meta name="author" content="Zihong Lin" />
    <meta name="description" content="" />
    <meta name="keywords" content="" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <link rel="icon" href="/images/avatar.jpg" />
    <link rel="preconnect" href="https://cdn.staticfile.org" />
<script src="https://cdn.staticfile.org/vue/3.3.7/vue.global.prod.min.js"></script>
<link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/6.4.2/css/all.min.css" />
<link rel="preconnect" href="https://fonts.loli.net" />
<link rel="preconnect" href="https://gstatic.loli.net" crossorigin />
<link rel="stylesheet" href="https://fonts.loli.net/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap" />
<script> const mixins = {}; </script>

<script src="https://polyfill.io/v3/polyfill.min.js?features=default"></script>


<script src="https://cdn.staticfile.org/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.org/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script>
<link
    rel="stylesheet"
    href="https://cdn.staticfile.org/highlight.js/11.9.0/styles/github.min.css"
/>
<script src="/js/lib/highlight.js"></script>



<script src="/js/lib/preview.js"></script>









<link rel="stylesheet" href="/css/main.css" />

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.1.1"></head>
<body>
    <div id="layout">
        <transition name="fade">
            <div id="loading" v-show="loading">
                <div id="loading-circle">
                    <h2>LOADING</h2>
                    <p>加载过慢请开启缓存 浏览器默认开启</p>
                    <img src="/images/loading.gif" />
                </div>
            </div>
        </transition>
        <div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}">
    <nav id="desktop-menu">
        <a class="title" href="/">
            <span>OKABE&#39;S LAB</span>
        </a>
        
        <a href="/">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;Home</span>
        </a>
        
        <a href="/about">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;About</span>
        </a>
        
        <a href="/archives">
            <i class="fa-solid fa-box-archive fa-fw"></i>
            <span>&ensp;Archives</span>
        </a>
        
        <a href="/categories">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;Categories</span>
        </a>
        
        <a href="/tags">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;Tags</span>
        </a>
        
    </nav>
    <nav id="mobile-menu">
        <div class="title" @click="showMenuItems = !showMenuItems">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;OKABE&#39;S LAB</span>
        </div>
        <transition name="slide">
            <div class="items" v-show="showMenuItems">
                
                <a href="/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-house fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Home</div>
                    </div>
                </a>
                
                <a href="/about">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-id-card fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">About</div>
                    </div>
                </a>
                
                <a href="/archives">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-box-archive fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Archives</div>
                    </div>
                </a>
                
                <a href="/categories">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-bookmark fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Categories</div>
                    </div>
                </a>
                
                <a href="/tags">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-tags fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Tags</div>
                    </div>
                </a>
                
            </div>
        </transition>
    </nav>
</div>
<transition name="fade">
    <div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div>
</transition>

        <div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'">
            <div class="article">
    <div>
        <h1>BERT 笔记</h1>
    </div>
    <div class="info">
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2022/12/1
        </span>
        
        <span class="category">
            <a href="/categories/AI/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                AI
            </a>
        </span>
        
        
        <span class="tags">
            <span class="icon">
                <i class="fa-solid fa-tags fa-fw"></i>
            </span>
            
            
            <span class="tag">
                
                <a href="/tags/BERT/" style="color: #ffa2c4">BERT</a>
            </span>
            
            <span class="tag">
                
                <a href="/tags/Transformer/" style="color: #ff7d73">Transformer</a>
            </span>
            
            <span class="tag">
                
                <a href="/tags/NLP/" style="color: #00a596">NLP</a>
            </span>
            
        </span>
        
    </div>
    
    <div class="content" v-pre>
        <p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805v2">https://arxiv.org/abs/1810.04805v2</a></p>
<p><img src="/2022/12/01/bert/BERT0.jpg"></p>
<blockquote>
<p>BERT是2018年10月由Google AI研究院提出的一种预训练模型。BERT的全称是Bidirectional Encoder Representation from Transformers。BERT在机器阅读理解顶级水平测试SQuAD1.1中表现出惊人的成绩: 全部两个衡量指标上全面超越人类，并且在11种不同NLP测试中创出SOTA表现，包括将GLUE基准推高至80.4% (绝对改进7.6%)，MultiNLI准确度达到86.7% (绝对改进5.6%)，成为NLP发展史上的里程碑式的模型成就。</p>
</blockquote>
<p>芝麻街大家族。</p>
<p><img src="/2022/12/01/bert/BERT1.png"></p>
<h2 id="BERT-的结构"><a href="#BERT-的结构" class="headerlink" title="BERT 的结构"></a>BERT 的结构</h2><p>沿用了 <code>Transformer</code> 的 <code>Encoder</code>。</p>
<p><img src="/2022/12/01/bert/BERT14.jpg"></p>
<p><img src="/2022/12/01/bert/BERT15.jpg"></p>
<h2 id="BERT-是怎么学做填空题的？"><a href="#BERT-是怎么学做填空题的？" class="headerlink" title="BERT 是怎么学做填空题的？"></a>BERT 是怎么学做填空题的？</h2><blockquote>
<p>BERT 的作者认为：使用两个方向（从左至右和从右至左）的单向编码器拼接而成的双向编码器，在性能、参数规模和效率等方面都不如直接使用双向编码器强大；这是 BERT 模型使用 Transformer Encoder 结构作为特征提取器，而不拼接使用两个方向的 Transformer Decoder 结构作为特征提取器的原因。</p>
</blockquote>
<blockquote>
<p>这也令 BERT 模型不能像 GPT 模型一样，继续使用标准语言模型的训练模式，因此 BERT 模型重新定义了两种模型训练方法（即：预训练任务）：MLM 和 NSP。BERT用MLM（Masked Language Model，掩码语言模型）方法训练词的语义理解能力，用NSP（Next Sentence Prediction，下句预测）方法训练句子之间的理解能力，从而更好地支持下游任务。</p>
</blockquote>
<h3 id="Masking-Input"><a href="#Masking-Input" class="headerlink" title="Masking Input"></a>Masking Input</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a></p>
<p>在给 <code>BERT</code> 数据的时候，将若干字替换成一个特殊的 <code>token</code>，或者随机的值。两种方式随机选择一种。</p>
<p><img src="/2022/12/01/bert/BERT3.png"></p>
<p>前面提到过，<code>BERT</code> 即是 <code>Encoder</code>，会输出一个向量的序列。 将这个向量经过一个线性层 + <code>softmax</code> 就可以得到遮住的字的概率分布。<br>我们既然已经知道“台”后面的字是“湾”，也就是说我们有标签，自然就可以用交叉熵作为损失函数训练咯~</p>
<p>这也就是“自己出题，自己回答”。</p>
<h3 id="Next-Sentence-Prediction"><a href="#Next-Sentence-Prediction" class="headerlink" title="Next Sentence Prediction"></a>Next Sentence Prediction</h3><p><code>SEP</code> 区分两个句子。</p>
<p>这边只看 <code>CLS</code> 对应的输出，经过线性变换，做一个二元分类问题（Yes&#x2F;No），即判断两个句子是不是相邻的。</p>
<h2 id="BERT-学到了什么？"><a href="#BERT-学到了什么？" class="headerlink" title="BERT 学到了什么？"></a>BERT 学到了什么？</h2><p>自监督学习。</p>
<p><img src="/2022/12/01/bert/BERT2.jpg"></p>
<p><code>BERT</code> 实际上学到了如何做填空题。</p>
<p><img src="/2022/12/01/bert/BERT5.png"></p>
<p>先预训练，然后在微调以适应不同的任务。就像一个干细胞，可以分化成任何人体细胞。</p>
<p><code>BERT</code> 本身是无监督的（自己出填空题，自己学着做），但是其下游任务（downstream task）可能是监督学习的。比如语句情感分类，利用之前学习填空题 pretrain<br>过的 <code>BERT</code> 模型，将其输出经过另一个网络（这个网络是需要投喂数据集监督学习的），最终输出情感分类。</p>
<p>pretrain 过的 <code>BERT</code> 会比随机参数的好。All you need is <strong>fine-tuning!</strong></p>
<p><img src="/2022/12/01/bert/BERT7.png"></p>
<p>对于词性识别，流程也是类似的。生成的每个向量都通过一个待训练网络，剩下的和上面的例子一样。</p>
<p><img src="/2022/12/01/bert/BERT8.jpg"></p>
<p>判断假设和推断是否矛盾：</p>
<p><img src="/2022/12/01/bert/BERT9.jpg"></p>
<p>输入文章和问题，返回两个整数（答案的起止位置）：</p>
<p><img src="/2022/12/01/bert/BERT10.jpg"></p>
<p>和自注意力机制类似，橙色部分类似于一个 <code>Query</code>，去查询 <code>document</code> 中的每一个 <code>Key</code>，最终经过 <code>softmax</code> 的到最大概率的作为 start。<br>这边 s &#x3D; 2。</p>
<p><img src="/2022/12/01/bert/BERT11.jpg"></p>
<p>还有一个蓝色的向量需要学习，对应于 end。和 start 类似，这边求出 e &#x3D; 3。于是我们得出了答案的范围。</p>
<p><img src="/2022/12/01/bert/BERT12.jpg"></p>
<p>注意橙色和蓝色向量都是要训练的对象。</p>
<h3 id="Training-BERT-is-challenging"><a href="#Training-BERT-is-challenging" class="headerlink" title="Training BERT is challenging"></a>Training BERT is challenging</h3><p>训练 <code>BERT</code> 需要极大的数据量和极强的硬件。</p>
<h3 id="预训练-Seq2Seq"><a href="#预训练-Seq2Seq" class="headerlink" title="预训练 Seq2Seq"></a>预训练 Seq2Seq</h3><p>类似的，也可以预训练 <code>Decoder</code>。将一些句子破坏掉（原始句子作为标签），进行 <code>self-supervise</code></p>
<p><img src="/2022/12/01/bert/BERT13.jpg"></p>
<h2 id="为什么-BERT-能够工作？"><a href="#为什么-BERT-能够工作？" class="headerlink" title="为什么 BERT 能够工作？"></a>为什么 BERT 能够工作？</h2><p><img src="/2022/12/01/bert/BERT18.jpg"></p>
<p>类似于 <code>word2vec</code>，<code>BERT</code> 能够将每个词都对应到一个向量（表征某个词的含义（meaning））。向量两两之间的距离代表了两个词的意思的接近程度。</p>
<p><img src="/2022/12/01/bert/BERT16.jpg"></p>
<p>但是不同的是，<code>BERT</code> 在预训练的时候会学习到上下文信息，同一个字在不同的上下文中，其含义不同，<code>BERT</code> 输出的向量也不同。所以 <code>BERT</code> 输出的<br><code>Embedding</code> 是动态的，会根据上下文的不同产生变化。</p>
<p>以 <code>苹果</code> 为例，“吃苹果”的“苹果”是一种水果，而“苹果手机”的“苹果”则是一个公司品牌名。</p>
<p>通过计算这两个苹果经过 <code>BERT</code> 输出的 <code>Embedding</code> 的 <code>cosine similarity</code>，可以得到下图：</p>
<p><img src="/2022/12/01/bert/BERT17.jpg"></p>
<h2 id="令人震惊的-BERT"><a href="#令人震惊的-BERT" class="headerlink" title="令人震惊的 BERT"></a>令人震惊的 BERT</h2><h3 id="DNA-classification"><a href="#DNA-classification" class="headerlink" title="DNA classification"></a>DNA classification</h3><p>用语言资料预训练的 <code>BERT</code> 甚至可以提升 <code>DNA</code> 分类的准确度！</p>
<p><img src="/2022/12/01/bert/BERT19.jpg"></p>
<p>因为预训练用的是英语，所以这边需要将碱基对对应到一个随机词汇上，然后再对 <code>DNA</code> 分类问题进行 <code>Fine-Tune</code>。即使是这样八竿子打不着的映射，<br>居然也能学出更好的效果——难道 <code>DNA</code> 序列结构和某种语言语法结构有关？ </p>
<p><img src="/2022/12/01/bert/BERT20.jpg"></p>
<h3 id="Multi-lingual-BERT"><a href="#Multi-lingual-BERT" class="headerlink" title="Multi-lingual BERT"></a>Multi-lingual BERT</h3><p>用多种语言的资料训练的 <code>BERT</code> 能够学习到语言之间的联系，在没有学习中文问答资料的情况下，输入中文问题居然也可以达到惊人的准确率。</p>
<p><img src="/2022/12/01/bert/BERT22.jpg"></p>
<p>似乎 <code>BERT</code> 在学到 <code>Rabbit</code> 和 <code>兔</code> 的向量距离很近的同时，也没有忘记语言类型信息。也就是在回答中文问题的时候，它不会将 <code>兔</code> 替换成 <code>Rabbit</code>，<br>而是都用中文来回答。</p>
<p><img src="/2022/12/01/bert/BERT23.jpg"></p>
<p>或许是因为，两个词汇虽然很接近，但是在它们所处的极高维空间内，它们根本不在同一个维度，只是距离相近，仅此而已。</p>
<p>如果将不同语言的所有词汇的 <code>Embedding</code> 求均值，将会发现这些语言在高维空间中散落在不同的区域。假如说你求出 <code>中文</code> 和 <code>English</code> 的这种距离，<br>然后将一个英文 <code>Embedding</code> 加上这个距离，你将会得到中文的 <code>Embedding</code>！</p>
<p><img src="/2022/12/01/bert/BERT24.jpg"></p>
<h1 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h1><p><img src="/GPT2.jpg"></p>
<h2 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h2><p>结构类似 <code>Transformer</code> 的 <code>Decoder</code>（使用 <code>Masked self-attention</code>）。</p>
<p><img src="/GPT1.jpg"></p>
<p><code>GPT</code> 和 <code>BERT</code> 不一样的地方在于 <code>GPT</code> 是在是太大了，以至于我们难以 <code>Fine-tune</code> 它。 </p>
<h3 id="In-context-learning"><a href="#In-context-learning" class="headerlink" title="In-context learning"></a>In-context learning</h3><p>这里不会去利用梯度下降更新 <code>GPT</code> 的参数。</p>
<h3 id="Few-shot-One-shot-Zero-shot-Learning"><a href="#Few-shot-One-shot-Zero-shot-Learning" class="headerlink" title="Few-shot&#x2F;One-shot&#x2F;Zero-shot Learning"></a>Few-shot&#x2F;One-shot&#x2F;Zero-shot Learning</h3><p>提供任务描述和例子，学会举一反三。</p>
<ul>
<li><p>Few-shot:</p>
<p>  <img src="/GPT3.jpg"></p>
</li>
<li><p>One-shot:</p>
<p>  <img src="/GPT4.jpg"></p>
</li>
<li><p>Zero-shot:</p>
<p>  <img src="/GPT5.jpg"></p>
</li>
</ul>
<h2 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h2><p><img src="/GPT6.jpg"></p>

    </div>
    
    
    
    
    <div id="comment">
        <div id="giscus-container" class="giscus"></div>
    </div>
    
    
    
    
</div>

            <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2022 - 2025 Okabe&#39;s LAB
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;Zihong Lin
        </div>
        <div>
            Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp;
            <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a>
        </div>
        
    </div>
</footer>

        </div>
        
        <transition name="fade">
            <div id="preview" ref="preview" v-show="previewShow">
                <img id="preview-content" ref="previewContent" />
            </div>
        </transition>
        
    </div>
    <script src="/js/main.js"></script>
    
    
<script
    src="https://giscus.app/client.js"
    data-repo="Okabe-Rintarou-0/Okabe-Rintarou-0.github.io"
    data-repo-id="R_kgDOJD3YPA"
    data-category="Announcements"
    data-category-id="DIC_kwDOJD3YPM4Cc9W2"
    data-mapping="pathname"
    data-strict="0"
    data-reactions-enabled="1"
    data-emit-metadata="0"
    data-input-position="bottom"
    data-theme="preferred_color_scheme"
    data-lang="zh-CN"
    crossorigin
    async
></script>





    
</body>
</html>
