
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <title>Okabe&#39;s LAB</title>
    <meta name="author" content="Zihong Lin" />
    <meta name="description" content="" />
    <meta name="keywords" content="" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <link rel="icon" href="/images/avatar.jpg" />
    <link rel="preconnect" href="https://cdn.staticfile.org" />
<script src="https://cdn.staticfile.org/vue/3.3.7/vue.global.prod.min.js"></script>
<link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/6.4.2/css/all.min.css" />
<link rel="preconnect" href="https://fonts.loli.net" />
<link rel="preconnect" href="https://gstatic.loli.net" crossorigin />
<link rel="stylesheet" href="https://fonts.loli.net/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap" />
<script> const mixins = {}; </script>

<script src="https://polyfill.io/v3/polyfill.min.js?features=default"></script>


<script src="https://cdn.staticfile.org/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.org/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script>
<link
    rel="stylesheet"
    href="https://cdn.staticfile.org/highlight.js/11.9.0/styles/github.min.css"
/>
<script src="/js/lib/highlight.js"></script>



<script src="/js/lib/preview.js"></script>





<script src="/js/lib/home.js"></script>

<link rel="stylesheet" href="/css/main.css" />

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.1.1"></head>
<body>
    <div id="layout">
        <transition name="fade">
            <div id="loading" v-show="loading">
                <div id="loading-circle">
                    <h2>LOADING</h2>
                    <p>加载过慢请开启缓存 浏览器默认开启</p>
                    <img src="/images/loading.gif" />
                </div>
            </div>
        </transition>
        <div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}">
    <nav id="desktop-menu">
        <a class="title" href="/">
            <span>OKABE&#39;S LAB</span>
        </a>
        
        <a href="/">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;Home</span>
        </a>
        
        <a href="/about">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;About</span>
        </a>
        
        <a href="/archives">
            <i class="fa-solid fa-box-archive fa-fw"></i>
            <span>&ensp;Archives</span>
        </a>
        
        <a href="/categories">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;Categories</span>
        </a>
        
        <a href="/tags">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;Tags</span>
        </a>
        
    </nav>
    <nav id="mobile-menu">
        <div class="title" @click="showMenuItems = !showMenuItems">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;OKABE&#39;S LAB</span>
        </div>
        <transition name="slide">
            <div class="items" v-show="showMenuItems">
                
                <a href="/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-house fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Home</div>
                    </div>
                </a>
                
                <a href="/about">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-id-card fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">About</div>
                    </div>
                </a>
                
                <a href="/archives">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-box-archive fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Archives</div>
                    </div>
                </a>
                
                <a href="/categories">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-bookmark fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Categories</div>
                    </div>
                </a>
                
                <a href="/tags">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-tags fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Tags</div>
                    </div>
                </a>
                
            </div>
        </transition>
    </nav>
</div>
<transition name="fade">
    <div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div>
</transition>

        <div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'">
            <div id="home-head">
    <div id="home-background" ref="homeBackground" data-images="/images/background.jpg"></div>
    <div id="home-info" @click="homeClick">
        <span class="loop"></span>
        <span class="loop"></span>
        <span class="loop"></span>
        <span class="loop"></span>
        <span class="info">
            <div class="wrap">
                <h1>Okabe&#39;s LAB</h1>
                <h3></h3>
                <h5></h5>
            </div>
        </span>
    </div>
</div>
<div id="home-posts-wrap" true ref="homePostsWrap">
    <div id="home-posts">
        

<div class="post">
    <a href="/2025/08/06/tla/">
        <h2 class="post-title">形式化验证之 TLA+ 入门</h2>
    </a>
    <div class="category-and-date">
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/8/6
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><pre><code class="bash">brew install --cask tla+-toolbox
</code></pre>
<h2 id="案例——判断死锁"><a href="#案例——判断死锁" class="headerlink" title="案例——判断死锁"></a>案例——判断死锁</h2><pre><code class="plaintext">------------------------------ MODULE deadlock ------------------------------

EXTENDS Integers
VARIABLES locks, locksA, locksB

Init == 
    /\ locks = &#123;&#125;
    /\ locksA = &#123;&#125;
    /\ locksB = &#123;&#125;

A_REQUIRE_LOCK_1 ==
    /\ ~(1 \in locks)
    /\ locks&#39; = locks \cup &#123;1&#125;
    /\ locksA&#39; = locksA \cup &#123;1&#125;
    /\ locksB&#39; = locksB

A_REQUIRE_LOCK_2 ==
    /\ 1 \in locksA
    /\ ~(2 \in locks)
    /\ locks&#39; = locks \cup &#123;2&#125;
    /\ locksA&#39; = locksA \cup &#123;2&#125;
    /\ locksB&#39; = locksB
    
B_REQUIRE_LOCK_1 ==
    /\ 2 \in locksB
    /\ ~(1 \in locks)
    /\ locks&#39; = locks \cup &#123;1&#125;
    /\ locksB&#39; = locksB \cup &#123;1&#125;
    /\ locksA&#39; = locksA

B_REQUIRE_LOCK_2 ==
    /\ ~(2 \in locks)
    /\ locks&#39; = locks \cup &#123;2&#125;
    /\ locksB&#39; = locksB \cup &#123;2&#125;
    /\ locksA&#39; = locksA

Next == A_REQUIRE_LOCK_1
    \/ A_REQUIRE_LOCK_2
    \/ A_REQUIRE_LOCK_1
    \/ A_REQUIRE_LOCK_2
    
Spec == Init /\ [][Next]_&lt;&lt;locks, locksA, locksB&gt;&gt;

=============================================================================
\* Modification History
\* Last modified Wed Aug 06 12:14:05 CST 2025 by lucas
\* Created Wed Aug 06 12:05:08 CST 2025 by lucas
</code></pre>
<p>本质上就是用 latex 的语法去写状态机的状态转移表达式，包括状态转移条件，以及下一个状态。注意每个状态都需要在状态转移表达式中声明（即便没有发生变化，如 <code>locksA&#39; locksA</code>）</p>
<p><img src="/2025/08/06/tla/image.png"></p>
<p><img src="/2025/08/06/tla/image-1.png"></p>
<p><img src="/2025/08/06/tla/image-2.png"></p>
<p><img src="/2025/08/06/tla/image-3.png"></p>
<p>成功检测到了死锁：<br><img src="/2025/08/06/tla/image-4.png"></p>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/TLA/" style="color: #ffa2c4">TLA+</a>
        </span>
        
    </div>
    <a href="/2025/08/06/tla/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2025/05/25/cpp-fix/">
        <h2 class="post-title">C++ 知识查漏补缺</h2>
    </a>
    <div class="category-and-date">
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/5/25
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h3 id="使用-std-tie-读取-std-pair"><a href="#使用-std-tie-读取-std-pair" class="headerlink" title="使用 std::tie 读取 std::pair"></a>使用 std::tie 读取 std::pair</h3><pre><code class="c++">std::pair&lt;int, double&gt; p&#123;1, 2.0&#125;;
int i;
double d;
std::tie(i, d) = p;
</code></pre>
<h3 id="std-atomic-is-always-lock-free"><a href="#std-atomic-is-always-lock-free" class="headerlink" title="std::atomic::is_always_lock_free"></a>std::atomic<T>::is_always_lock_free</h3><p>判断是否当前类型可以使用无锁原子实现。</p>
<h3 id="std-span"><a href="#std-span" class="headerlink" title="std::span"></a>std::span</h3><p><code>std::span</code> 就是一个连续对象存储的观察者。类似<code>std::string_view</code> 是 <code>std::string</code> 的观察者。</p>
<pre><code class="c++">int printSize(std::span&lt;int&gt; s) &#123; std::cout &lt;&lt; s.size() &lt;&lt; std::endl; &#125;

int main() &#123;
  std::vector&lt;int&gt; v&#123;1, 2, 3&#125;;
  int a[10];
  std::array&lt;int, 30&gt; arr;
  printSize(v); // 3
  printSize(a); // 10
  printSize(arr); // 30
&#125;
</code></pre>
<h3 id="std-vector-释放内存"><a href="#std-vector-释放内存" class="headerlink" title="std::vector 释放内存"></a>std::vector 释放内存</h3><p><code>std::vector::clear</code> 并不会释放 <code>capacity</code>，需要使用 <code>std::vector::shrink_to_fit</code> 或：</p>
<pre><code class="c++">std::vector&lt;int&gt; v&#123;1, 2, 3&#125;;
std::vector&lt;int&gt;().swap(v);
std::cout &lt;&lt; v.capacity() &lt;&lt; std::endl; // 0
</code></pre>
<h3 id="虚函数表占的空间"><a href="#虚函数表占的空间" class="headerlink" title="虚函数表占的空间"></a>虚函数表占的空间</h3><pre><code class="c++">class A &#123;
public:
    virtual void vfunc1();
    virtual void vfunc2();
    void func1();
    void func2();
private:
    int m_data1, m_data2;
&#125;;
</code></pre>
<p><img src="/2025/05/25/cpp-fix/vtable1.png"></p>
<p><img src="/2025/05/25/cpp-fix/vtable2.png"></p>
<h3 id="手写一个-std-function"><a href="#手写一个-std-function" class="headerlink" title="手写一个 std::function"></a>手写一个 std::function</h3><pre><code class="c++">#include &lt;iostream&gt;

template &lt;typename R, typename... Args&gt; struct ICallable &#123;
  virtual R invoke(Args &amp;&amp;...args) = 0;
  virtual ~ICallable() &#123;&#125;
&#125;;

template &lt;typename T, typename R, typename... Args&gt;
class CallableImpl : public ICallable&lt;R, Args...&gt; &#123;
private:
  T callable;

public:
  explicit CallableImpl(T callable) : callable(callable) &#123;&#125;

  R invoke(Args &amp;&amp;...args) override &#123;
    return callable(std::forward&lt;Args&gt;(args)...);
  &#125;
&#125;;

template &lt;typename Signature&gt; class Function;

template &lt;typename R, typename... Args&gt; class Function&lt;R(Args...)&gt; &#123;
private:
  std::unique_ptr&lt;ICallable&lt;R, Args...&gt;&gt; callable;

public:
  template &lt;typename T&gt;
  Function&lt;R(Args...)&gt;(T &amp;&amp;callable)
      : callable(std::make_unique&lt;CallableImpl&lt;T, R, Args...&gt;&gt;(
            std::forward&lt;T&gt;(callable))) &#123;&#125;

  Function(const Function&lt;R(Args...)&gt; &amp;other) = delete;
  Function &amp;operator=(const Function&lt;R(Args...)&gt; &amp;other) = delete;

  R operator()(Args &amp;&amp;...args) &#123;
    return callable-&gt;invoke(std::forward&lt;Args&gt;(args)...);
  &#125;
&#125;;

int mmax(int a, int b) &#123; return a &gt; b ? a : b; &#125;

int main() &#123;
  Function&lt;int(int, int)&gt; f1 = mmax;
  std::cout &lt;&lt; f1(3, 4) &lt;&lt; std::endl;

  Function&lt;int(int, int)&gt; f2 = std::function&lt;int(int, int)&gt;(mmax);
  std::cout &lt;&lt; f2(3, 4) &lt;&lt; std::endl;

  Function&lt;int(int, int)&gt; f3([](int a, int b) &#123; return a &gt; b ? a : b; &#125;);
  std::cout &lt;&lt; f3(3, 4) &lt;&lt; std::endl;

  class Callable &#123;
  public:
    int operator()(int x, int y) &#123; return x &gt; y ? x : y; &#125;
  &#125; c;
  Function&lt;int(int, int)&gt; f4(c);
  std::cout &lt;&lt; f4(3, 4) &lt;&lt; std::endl;
&#125;
</code></pre>
<h3 id="智能指针"><a href="#智能指针" class="headerlink" title="智能指针"></a>智能指针</h3><p><code>std::make_shared</code> 优于 <code>std::shared_ptr&lt;T&gt;(new T)</code> 的原因是：</p>
<ol>
<li>更简洁</li>
<li><code>std::make_shared</code> 会分配一块连续的空间（控制块 + 数据块），locality 更好。而后者的内存空间是分散的。</li>
</ol>
<p>手写 <code>std::shared_ptr</code>:</p>
<pre><code class="c++">template &lt;typename T&gt; class ReferenceCount &#123;
private:
  std::atomic&lt;size_t&gt; mRefCount;
  T *data;

public:
  ReferenceCount(T *data) : data(data), mRefCount(1) &#123;&#125;
  void increase() &#123; mRefCount.fetch_add(1); &#125;
  void decrease() &#123;
    if (mRefCount.fetch_sub(1) == 1) &#123;
      delete this;
    &#125;
  &#125;
  size_t count() const &#123; return mRefCount.load(); &#125;
&#125;;

template &lt;typename T&gt; class SharedPtr &#123;
private:
  T *data;
  ReferenceCount&lt;T&gt; *mRefCount;

public:
  SharedPtr(T *data) : data(data), mRefCount(new ReferenceCount&lt;T&gt;(data)) &#123;&#125;

  SharedPtr(const SharedPtr&lt;T&gt; &amp;other)
      : data(other.data), mRefCount(other.mRefCount) &#123;
    mRefCount-&gt;increase();
  &#125;

  SharedPtr&lt;T&gt; &amp;operator=(const SharedPtr&lt;T&gt; &amp;other) &#123;
    if (mRefCount != other.mRefCount) &#123;
      mRefCount-&gt;decrease();
      mRefCount = other.mRefCount;
      data = other.data;
    &#125;
    mRefCount-&gt;increase();
    return *this;
  &#125;

  size_t useCount() const &#123; return mRefCount-&gt;count(); &#125;

  T *operator-&gt;() &#123; return data; &#125;
&#125;;

template &lt;typename T, typename... Args&gt; SharedPtr&lt;T&gt; makeShared(Args... args) &#123;
  return SharedPtr&lt;T&gt;(new T(std::forward&lt;Args&gt;(args)...));
&#125;

int main() &#123;
  SharedPtr&lt;int&gt; a = makeShared&lt;int&gt;(4);
  SharedPtr&lt;int&gt; e = makeShared&lt;int&gt;(5);
  std::cout &lt;&lt; a.useCount() &lt;&lt; std::endl;
  SharedPtr&lt;int&gt; b = a;
  std::cout &lt;&lt; a.useCount() &lt;&lt; std::endl;
  SharedPtr&lt;int&gt; c(a);
  std::cout &lt;&lt; a.useCount() &lt;&lt; std::endl;

  b = e;
  std::cout &lt;&lt; a.useCount() &lt;&lt; std::endl;
  std::cout &lt;&lt; e.useCount() &lt;&lt; std::endl;

  struct Person &#123;
    Person(std::string name, int age) : name(name), age(age) &#123;&#125;
    std::string name;
    int age;
  &#125;;

  SharedPtr&lt;Person&gt; p = makeShared&lt;Person&gt;(&quot;Alice&quot;, 32);
  std::cout &lt;&lt; p-&gt;name &lt;&lt; &quot;, &quot; &lt;&lt; p-&gt;age &lt;&lt; std::endl;
&#125;
</code></pre>
<p>输出：</p>
<pre><code class="plaintext">1
2
3
2
2
Alice, 32
</code></pre>
<h3 id="手写-std-any"><a href="#手写-std-any" class="headerlink" title="手写 std::any"></a>手写 <code>std::any</code></h3><p>老样子，使用类型擦除技术（基于特化）。这里的思路我总结下来是这样：<br><code>std::any</code> 他是不需要指定类型的，比如：</p>
<pre><code class="c++">std::any a = 123;
</code></pre>
<p>这里实际上只有在构造的时候才会用到类型，对应一个特化的构造函数：</p>
<pre><code class="c++">template&lt;typename T&gt;
Any(T val);
</code></pre>
<p>例如：</p>
<pre><code class="c++">std::any a = 123;
std::any a = std::string(&quot;Hello&quot;);
</code></pre>
<p>将对应两个特化的构造函数。</p>
<p>既然 <code>std::any</code> 不需要指明其存储的类型 <code>T</code>，那么他的所有成员变量一定是和类型 <code>T</code> 无关的，但是我们总要有个地方存这个 <code>T</code> 类型的 value。</p>
<p>所以可以想到，我们定义一个接口，该接口和 <code>T</code> 类型无关，其实现类可以与 <code>T</code> 有关，该实现类是真正存储 <code>T</code> 类型 value 的地方。而 <code>std::any</code> 则可以存储这个接口。</p>
<pre><code class="c++">struct IValueHolder &#123;
  virtual const std::type_info &amp;type() const = 0;
&#125;;

template &lt;typename T&gt; class ValueHolder : public IValueHolder &#123;
private:
  T val;

public:
  explicit ValueHolder(T val) : val(val) &#123;&#125;

  const std::type_info &amp;type() const override &#123; return typeid(val); &#125;

  inline const T &amp;value() const &#123; return val; &#125;
&#125;;

class Any &#123;
private:
  std::unique_ptr&lt;IValueHolder&gt; holder;

public:
  template &lt;typename T&gt;
  Any(T val) : holder(std::make_unique&lt;ValueHolder&lt;T&gt;&gt;(val)) &#123;&#125;

  template &lt;typename T&gt; T AnyCast() &#123;
    if (typeid(T) != holder-&gt;type()) &#123;
      throw std::bad_cast();
    &#125;
    return static_cast&lt;ValueHolder&lt;T&gt; *&gt;(holder.get())-&gt;value();
  &#125;
&#125;;

int main() &#123;
  Any a = 123;
  std::cout &lt;&lt; a.AnyCast&lt;int&gt;() &lt;&lt; endl;
  std::cout &lt;&lt; a.AnyCast&lt;double&gt;() &lt;&lt; endl;
&#125;
</code></pre>
<p>输出：</p>
<pre><code class="plaintext">123
libc++abi: terminating due to uncaught exception of type std::bad_cast: std::bad_cast
[1]    25963 abort      /Users/lucas/projects/c++/bin/my_test
</code></pre>
<h3 id="enum-class"><a href="#enum-class" class="headerlink" title="enum class"></a>enum class</h3><ol>
<li>避免命名空间冲突。</li>
<li>可以指定底层存储的类型，<code>int</code> or <code>long long</code>…</li>
<li>可以前置声明。</li>
</ol>
<h3 id="std-atomic"><a href="#std-atomic" class="headerlink" title="std::atomic"></a><code>std::atomic</code></h3><blockquote>
<p>std::atomic 背后的缓存行锁定（Cache Line Locking）是实现原子操作的关键机制，它依赖于现代 CPU 的缓存一致性协议（如 MESI）和总线锁定（Bus Locking）。当线程对 std::atomic 变量执行原子操作时，CPU 首先尝试通过缓存行锁定实现高效并发：如果该变量所在的缓存行被当前 CPU 独占（Modified 状态），则直接修改并标记为脏；若被其他 CPU 共享（Shared 状态），则通过总线发送 invalidate 信号，迫使其他 CPU 放弃缓存行的所有权，从而获取独占权后执行操作。若缓存行锁定失败（如跨缓存行的操作），CPU 会退化为总线锁定，临时阻塞整个内存总线，确保操作的原子性。这种分级机制平衡了性能与正确性，使 std::atomic 在大多数场景下仅需短暂的缓存行竞争，避免了昂贵的总线锁定开销。</p>
</blockquote>
<pre><code class="c++">std::atomic&lt;int&gt; flag&#123;0&#125;;
int data = 0;

// 线程 A（写端）
data = 42;                          // 非原子写入
flag.store(1, std::memory_order_release); // release 保证 data 的写入不会重排到之后

// 线程 B（读端）
if (flag.load(std::memory_order_acquire) == 1) &#123; // acquire 保证读到 release 的写入
    assert(data == 42); // 一定成功！data 的写入对线程 B 可见
&#125;
</code></pre>
<p><code>std::memory_order_release</code> 保证它之前的所有读写都不会重排到它后面，而 <code>std::memory_order_acquire</code> 又保证了它之后的所有读写不会重排到它前面，这样就形成了 <code>release-acquire</code> 模式，前者保证 <code>flag.Store</code> 之前 <code>data = 42</code> 已经写入；后者保证 <code>flag.load</code> 之后，<code>data = 42</code> 才被读取。 </p>
<h3 id="noexcept"><a href="#noexcept" class="headerlink" title="noexcept"></a><code>noexcept</code></h3><p><code>std::vector</code> 扩缩容的时候，如果元素的移动构造函数被标记为 <code>noexcept</code>，那它就会大胆地调用移动构造函数，否则就会使用相对更安全的拷贝构造函数。<code>std::sort</code> 的优化类似。</p>
<pre><code class="c++">class A &#123;
public:
  A() = default;
  A(const A &amp;) &#123; std::cout &lt;&lt; &quot;copy&quot; &lt;&lt; std::endl; &#125;

  A(A &amp;&amp;) &#123; std::cout &lt;&lt; &quot;move&quot; &lt;&lt; std::endl; &#125;
&#125;;

int main() &#123;
  std::vector&lt;A&gt; aVec;
  aVec.emplace_back();
  aVec.emplace_back();
&#125;
</code></pre>
<p>输出：</p>
<pre><code class="plaintext">copy
</code></pre>
<p>将移动构造函数标记为 <code>noexcept</code>，输出：</p>
<pre><code class="plaintext">move
</code></pre>
<h3 id="std-enable-shared-from-this"><a href="#std-enable-shared-from-this" class="headerlink" title="std::enable_shared_from_this"></a><code>std::enable_shared_from_this</code></h3><p>有些时候，我们需要在一个 <code>std::shared_ptr</code> 托管的类的成员函数中，创建它的一个 <code>shared_ptr</code> 新实例，传给某些函数，例如：</p>
<pre><code class="c++">class Manager;
class Component &#123;
private:
  int *value = new int(3);
  Manager *mgr;

public:
  Component(Manager *mgr) : mgr(mgr) &#123;&#125;
  ~Component() &#123;
    std::cout &lt;&lt; &quot;Deconstruct&quot; &lt;&lt; std::endl;
    delete value;
  &#125;
  void DoSomething();
  friend class Manager;
&#125;;

class Manager &#123;
public:
  void Modify(std::shared_ptr&lt;Component&gt; c) &#123; *c-&gt;value = 3; &#125;
&#125;;

void Component::DoSomething() &#123; mgr-&gt;Modify(std::shared_ptr&lt;Component&gt;(this)); &#125;

int main() &#123;
  Manager mgr;
  auto c = std::make_shared&lt;Component&gt;(&amp;mgr);
  c-&gt;DoSomething();
&#125;
</code></pre>
<p>上述的代码会导致 <code>double free</code> 的问题，正确做法如下：</p>
<pre><code class="c++">class Component : public std::enable_shared_from_this&lt;Component&gt; &#123;
  ...
&#125;
void Component::DoSomething() &#123; mgr-&gt;Modify(shared_from_this()); &#125;
</code></pre>
<p><img src="/2025/05/25/cpp-fix/enable_shared_from_this.png"></p>
<p>其原理是：构造 <code>Component</code> 的 <code>shared_ptr</code> 的时候，会同时定义一个 <code>weak_ptr</code> 指向 <code>shared_ptr</code>。在调用 <code>shared_from_this</code> 的时候，会由该 <code>weak_ptr</code> 创建对应的 <code>shared_ptr</code>。</p>
<p>注意，此处的 <code>std::enable_shared_from_this&lt;Component&gt;</code> 必须是 <code>public</code> 继承，不然创建 <code>std::shared_ptr</code> 的时候无法正确访问 <code>enable_shared_from_this</code> 中的 <code>weak_ptr</code>。</p>
<h3 id="std-deque-实现原理"><a href="#std-deque-实现原理" class="headerlink" title="std::deque 实现原理"></a><code>std::deque</code> 实现原理</h3><p><code>std::deque</code> 通常采用 分块存储（Chunked Storage） 的方式实现，即：<br>由多个固定大小的 “块（Chunks）” 组成，每个块是一个连续的内存数组（称为 “缓冲区” 或 “页”）。</p>
<p>使用一个中央控制结构（如指针数组或动态数组）管理这些块，记录每个块的地址。<br>这种设计使得：<br>头尾插入&#x2F;删除高效（不需要像 <code>vector</code> 那样整体移动元素）。<br>随机访问（<code>operator[]</code>）比 <code>list</code> 快（计算块索引 + 块内偏移）。</p>
<p>典型的 std::deque 实现（如 GCC 的 libstdc++）采用 “Map + Chunks” 结构：</p>
<pre><code class="plaintext">+-------------------+     +-------------------+     +-------------------+
| Chunk 0 (front)   | --&gt; | Chunk 1           | --&gt; | Chunk N (back)    |
+-------------------+     +-------------------+     +-------------------+
| [0] | [1] | ...   |     | [0] | [1] | ...   |     | [0] | [1] | ...   |
+-------------------+     +-------------------+     +-------------------+
</code></pre>
<ul>
<li>Map（控制中心）：一个动态数组（如 T**），存储指向各个块的指针。</li>
<li>Chunk（块）：每个块是一个固定大小的数组（如 512B 或 4KB），存储实际数据。</li>
</ul>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/C/" style="color: #03a9f4">C++</a>
        </span>
        
    </div>
    <a href="/2025/05/25/cpp-fix/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2025/05/20/csi/">
        <h2 class="post-title">Kubernetes CSI</h2>
    </a>
    <div class="category-and-date">
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/5/20
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <ol>
<li>PVC（Persistent Volume Claim，持久卷声明）：是用户对存储资源的请求，用于向 Kubernetes 申请存储，抽象化存储的使用细节，方便应用获取和使用持久化存储。</li>
<li>PV（Persistent Volume，持久卷）：是 Kubernetes 中底层存储的抽象，代表具体的存储资源（如 NFS、云硬盘等），通过定义存储的类型、容量、访问模式等参数，供 PVC 动态绑定调用。</li>
<li>StorageClass：用于定义存储资源的配置模板，可动态创建 PV，支持按需提供存储服务，通过参数配置存储的类型（如 SSD、HDD）、备份策略等，实现存储资源的自动化管理和动态供给。</li>
</ol>
<h3 id="静态绑定"><a href="#静态绑定" class="headerlink" title="静态绑定"></a>静态绑定</h3><p>以<a target="_blank" rel="noopener" href="https://help.aliyun.com/zh/ack/ack-managed-and-ack-dedicated/user-guide/mount-statically-provisioned-nas-volumes?spm=a2c4g.11186623.help-menu-85222.d_2_4_4_0.74ed6a35zKrD6X&scm=20140722.H_134884._.OR_help-T_cn~zh-V_1">使用NAS静态存储卷</a>为例：</p>
<ul>
<li><p>PV</p>
<p>  静态绑定的方式需要手动配置 <code>PV</code>，并设定 <code>labels</code>：</p>
<pre><code class="yaml">apiVersion: v1
kind: PersistentVolume
metadata:
name: pv-nas
labels:
    alicloud-pvname: pv-nas
spec:
capacity:
    storage: 5Gi
accessModes:
    - ReadWriteMany
csi:
    driver: nasplugin.csi.alibabacloud.com
    volumeHandle: pv-nas   # 必须与PV Name保持一致。
    volumeAttributes:
    server: &quot;0c47****-mpk25.cn-shenzhen.nas.aliyuncs.com&quot;  # NAS挂载点，与集群VPC一致。
    path: &quot;/csi&quot;  # 挂载子目录。
mountOptions:
- nolock,tcp,noresvport
- vers=3
</code></pre>
</li>
<li><p>PVC</p>
<p>  <code>PVC</code> 通过 <code>label selector</code> 绑定上面手动配置的 <code>PV</code>。</p>
</li>
</ul>
<h3 id="动态绑定"><a href="#动态绑定" class="headerlink" title="动态绑定"></a>动态绑定</h3><ol>
<li>用户创建 PVC：用户通过 PVC 声明存储需求（如容量、访问模式、存储类型等），但此时无对应的 PV 绑定。</li>
<li>StorageClass 匹配与创建：PVC 会根据指定的 StorageClass （或默认类），触发存储插件（如云厂商存储插件、NFS 插件等）动态创建符合要求的 PV，并将两者绑定。</li>
<li>应用使用存储：绑定完成后，Pod 可通过 PVC 挂载自动创建的 PV，实现按需获取存储资源，无需手动预先创建 PV。</li>
</ol>
<p><img src="/2025/05/20/csi/dynamic_1.png"></p>
<p><img src="/2025/05/20/csi/dynamic_2.png"></p>
<p><code>PVC</code> 和 <code>PV</code> 以 <code>storageClassName</code> 为桥梁，以 <code>StorageClass</code> 为中介动态绑定。若 <code>storageClassName</code> 为空，则 <code>PVC</code> 将会和默认的 <code>PV</code> 绑定。</p>
<p><code>StorageClass</code> 的 <code>allowVolumeExpansion</code> 属性用于表明该存储是否允许扩容卷，以<a target="_blank" rel="noopener" href="https://help.aliyun.com/zh/ack/ack-managed-and-ack-dedicated/user-guide/mount-a-dynamically-provisioned-nas-volume?spm=a2c4g.11186623.help-menu-85222.d_2_4_4_1.44fd7a53svJPOn&scm=20140722.H_144398._.OR_help-T_cn~zh-V_1">阿里云 NAS</a>为例，若该属性为 <strong>true</strong>，则可以通过 patch <code>PVC</code>，修改其声明的卷大小来动态扩缩容。</p>
<h3 id="CSI-Driver"><a href="#CSI-Driver" class="headerlink" title="CSI Driver"></a>CSI Driver</h3><p><a target="_blank" rel="noopener" href="https://discuss.kubernetes.io/t/understanding-csi-architecture-and-communication/9404/2">Understanding CSI architecture and communication</a></p>
<p><img src="/2025/05/20/csi/csi.png"></p>
<ul>
<li>Daemonset Pod 应用可在所有节点上运行。  </li>
<li>StatefulSet &#x2F; Deployment Pod 应用作为 Kubernetes controller 运行。  </li>
<li>在 Daemonset Pod中，CSI Driver 通过 gRPC 和 socket 与 Kubelet 通信。  </li>
<li><code>node-driver-registrar</code> 将 CSI Driver 作为插件注册到 Kubelet。<br><a target="_blank" rel="noopener" href="https://speakerdeck.com/bells17/kubernetes-and-csi?slide=44">https://speakerdeck.com/bells17/kubernetes-and-csi?slide=44</a>  </li>
<li>在 StatefulSet &#x2F; Deployment Pod 中，CSI Driver 通过 gRPC 和 socket 与 CSI sidecar 应用通信。  </li>
<li>CSI sidecar 应用作为 Kubernetes controller 运行。  </li>
<li>例如，创建 PVC 资源时，<code>external-provisioner</code> 会监听该事件，并调用 CSI Driver 的 <code>CreateVolume</code> rpc。<br><a target="_blank" rel="noopener" href="https://speakerdeck.com/bells17/kubernetes-and-csi?slide=47">https://speakerdeck.com/bells17/kubernetes-and-csi?slide=47</a></li>
</ul>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/Kubernetes/" style="color: #00a596">Kubernetes</a>
        </span>
        
        <span class="tag">
            
            <a href="/tags/CSI/" style="color: #ffa2c4">CSI</a>
        </span>
        
    </div>
    <a href="/2025/05/20/csi/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2025/05/14/quantization/">
        <h2 class="post-title">几种模型量化的方式</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/AI/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                AI
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/5/14
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h3 id="标量量化（Scalar-Quantization）"><a href="#标量量化（Scalar-Quantization）" class="headerlink" title="标量量化（Scalar Quantization）"></a>标量量化（Scalar Quantization）</h3><p>FP32 -&gt; FP16，FP16 -&gt; INT8</p>
<p><img src="/2025/05/14/quantization/quantization.png"></p>
<p>以 FP16 -&gt; INT8 为例，将 FP16 的数值映射为 INT8 的范围。如果 tensor 中存在离群值（远大于平均的值），在 <code>de-Quantized</code> 之后可能导致其他值消失（接近 0）。解决方法是把这部分离群值拎出来，不量化，按照原有精度处理。</p>
<h3 id="乘积量化（Production-Quantization）"><a href="#乘积量化（Production-Quantization）" class="headerlink" title="乘积量化（Production Quantization）"></a>乘积量化（Production Quantization）</h3><p><img src="/2025/05/14/quantization/pq.png"></p>
<p>将原向量拆分成若干子向量，找到子向量距离最近的 <code>cluster</code>（聚类）对应的标识。由这些标识构成量化后的向量。</p>
<p>以上图为例，假设绿色为 0，蓝色为 1，红色为 2，则量化后的向量为：<br>[0, 1, 1, 2]。</p>
<h3 id="二进制量化与-RaBitQ"><a href="#二进制量化与-RaBitQ" class="headerlink" title="二进制量化与 RaBitQ"></a>二进制量化与 <a target="_blank" rel="noopener" href="https://github.com/gaoj0017/RaBitQ">RaBitQ</a></h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/7193968541">https://zhuanlan.zhihu.com/p/7193968541</a></p>
<p>二进制量化直接将向量的每一维度映射到一个 bit（0 或 1），常见的做法就是将大于 0 的维度设置为 1，否则则为 0。然而这种做法虽然极大地压缩了空间，但是也会显著影响召回率。</p>
<p><img src="/2025/05/14/quantization/rabitq.png"></p>
<p>RabitQ 通过将向量进行归一化，将其放入一个单位高维球中，这个单位高维球中有一个 <code>D</code> 维的超正方体（<code>D</code> 为向量维度）。靠近哪个顶点，就被划定为这个顶点，以这个顶点对应的向量作为量化结果。</p>
<p><img src="/2025/05/14/quantization/code_book.png"></p>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/AI/" style="color: #ffa2c4">AI</a>
        </span>
        
    </div>
    <a href="/2025/05/14/quantization/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2025/05/13/rocksdb-source/">
        <h2 class="post-title">RocksDB 源码阅读——SkipList</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/Database/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                Database
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/5/13
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/facebook/rocksdb/blob/ef67339175c1aebe039c0671560b608e22656612/memtable/inlineskiplist.h">RocksDB 源码 inlineskiplist.h</a></li>
</ul>
<h3 id="SkipList-Node-的实现"><a href="#SkipList-Node-的实现" class="headerlink" title="SkipList::Node 的实现"></a>SkipList::Node 的实现</h3><p>RocksDB 分配了一块连续的空间来存储节点在不同 level 的 <code>next</code> 指针。同时，使用 <code>std::atomic</code> 保证可以在无锁的情况下更新链表。其伪代码如下所示，可以说是把内存空间利用得严严实实。</p>
<pre><code class="c++">#include &lt;atomic&gt;
#include &lt;cstring&gt;
#include &lt;memory&gt;

// https://github.com/facebook/rocksdb/blob/ef67339175c1aebe039c0671560b608e22656612/memtable/inlineskiplist.h
class SkipList &#123;
public:
  struct Node &#123;
  private:
    std::atomic&lt;Node *&gt; next_[1];

  public:
    void StashHeight(size_t h) &#123; memcpy(&amp;next_[0], &amp;h, sizeof(h)); &#125;

    size_t UnstashHeight() &#123;
      size_t rv;
      memcpy(&amp;rv, &amp;next_[0], sizeof(rv));
      return rv;
    &#125;

    Node *Next(size_t level) &#123;
      return (&amp;next_[0] - level)-&gt;load(std::memory_order_acquire);
    &#125;
  &#125;;

  Node *AllocateNode(size_t height, size_t key_size) &#123;
    size_t prefix = sizeof(std::atomic&lt;Node *&gt;) * height - 1;
    char *raw = new char[prefix + sizeof(Node) + key_size];
    Node *node = reinterpret_cast&lt;Node *&gt;(raw + prefix);

    node-&gt;StashHeight(height);

    return node;
  &#125;
&#125;;
</code></pre>
<p>此处有个很有意思的点，在 <code>AllocateNode</code> 生成一个随机高度的节点的时候，会把他的高度通过 <code>StashHeight</code> 的方式暂时存储在 <code>next_[0]</code> 中。在后续的操作中，可以通过 <code>UnstashHeight</code> 读取存储的高度，并进行后续的 <code>next</code> 指针的更新（毕竟这些更新需要知道高度），可以说是不浪费任何内存空间。 </p>
<p>其内存布局大概如下图所示（此处我做了简化，<code>&amp;next_[0] - n</code> 其实就是相当于 <code>next_[-n]</code>，但是数组不支持负数）：</p>
<p><img src="/2025/05/13/rocksdb-source/memory.png"></p>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/Database/" style="color: #03a9f4">Database</a>
        </span>
        
        <span class="tag">
            
            <a href="/tags/RocksDB/" style="color: #00bcd4">RocksDB</a>
        </span>
        
    </div>
    <a href="/2025/05/13/rocksdb-source/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2025/05/10/vectordb/">
        <h2 class="post-title">Vector DB</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/AI/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                AI
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/5/10
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ul>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=YpulM5HdSr8">[FIXME][EP09] 向量数据库和出海创业</a></li>
<li><a target="_blank" rel="noopener" href="https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing/">Locality Sensitive Hashing (LSH): The Illustrated Guide</a></li>
<li><a target="_blank" rel="noopener" href="https://skyzh.github.io/write-you-a-vector-db/">Write You a Vector Database</a></li>
</ul>
<h3 id="常用的向量检索算法"><a href="#常用的向量检索算法" class="headerlink" title="常用的向量检索算法"></a>常用的向量检索算法</h3><ol>
<li><p>量化（以一定的模型性能换取查询效率），<a target="_blank" rel="noopener" href="https://github.com/gaoj0017/RaBitQ">RabitQ</a>。</p>
</li>
<li><p>Locality Sensitive Hashing (LSH)：传统的 hash 函数一般都希望减少碰撞，将一个 key 映射到尽可能分散的空间，而 <code>LSH</code> 则恰恰相反，通过将比较近的 <code>embedding</code> 映射到尽可能相同的 <code>bucket</code> 中，这样就能快速找到相邻的节点。</p>
<p> <img src="/2025/05/10/vectordb/lsh.png"></p>
</li>
<li><p>NSW(Navigate Small World)</p>
<p> <img src="/2025/05/10/vectordb/nsw.png"></p>
<p> 选择若干个 <code>entry</code>，然后维护一个有固定大小上限的队列，以类似 BFS 的方式，将距离目标向量最近的向量入队。需要多个 <code>entry</code> 的原因是，如果只采用一个，很容易陷入局部最优解。</p>
<p> 为了加速查询，我们会限制图中点之间的边的数量为 <code>max_m</code>。当我们插入一个节点的时候，其邻居节点的边可能会超过 <code>max_m</code>，所以我们需要重新计算这些节点的最近邻。</p>
</li>
<li><p>HNSW(Hierarchical Navigate Small World)</p>
<p> 将图以类似跳表的形式组织，但是无论是 NSW 还是 HNSW 都会涉及大量磁盘随机 I&#x2F;O，性能受限。</p>
</li>
<li><p>IVF(Inverted File)</p>
<p> <img src="/2025/05/10/vectordb/ivf.png"></p>
<p> 使用 <code>Kmeans</code> 将向量分为多个 <code>cluster</code>。并根据 <code>cluster</code> 的标识建立对向量的<code>倒排索引</code>。由于索引存储在一起，可以被顺序读写。</p>
<p> 在查询的时候，除了判断距离查询向量最近的质心对应的集群，还需要判断临近的集群。因为集群的质心可能距离查询向量比较远，但是边缘的点可能比较近。</p>
<p> <img src="/2025/05/10/vectordb/ivf_search.png"></p>
</li>
</ol>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/AI-Infra/" style="color: #00a596">AI Infra</a>
        </span>
        
    </div>
    <a href="/2025/05/10/vectordb/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2025/04/28/vllm/">
        <h2 class="post-title">vllm 学习笔记</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/AI/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                AI
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/4/28
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/717581669">大模型推理加速与KV Cache（一）：什么是KV Cache</a></li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=W83Zgbg8SkE&list=PLJj_urhaf2_qxpg8A5-6xoMvMLBKQMTX1&index=9">[FIXME][EP02][直播版] vllm源码讲解，分布式推理</a></li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=mWvqA_BNtsU&list=PLJj_urhaf2_qxpg8A5-6xoMvMLBKQMTX1">[FIXME][EP05][直播版] vllm从开源到部署，Prefix Caching和开源答疑</a></li>
<li><a target="_blank" rel="noopener" href="https://kevincheung2259.github.io/2025/03/29/CacheBlend/">CacheBlend-高效提高KVCache复用性的方法</a></li>
</ol>
<h3 id="Distributed-Inference"><a href="#Distributed-Inference" class="headerlink" title="Distributed Inference"></a>Distributed Inference</h3><p>具体的通信代码可以参考：<a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/llama.py">vllm&#x2F;model_executor&#x2F;models&#x2F;llama.py</a></p>
<h4 id="分布式通信手段"><a href="#分布式通信手段" class="headerlink" title="分布式通信手段"></a>分布式通信手段</h4><ol>
<li>nvlink</li>
<li>InfiniBand</li>
<li>RDMA</li>
</ol>
<h4 id="并行方式"><a href="#并行方式" class="headerlink" title="并行方式"></a>并行方式</h4><ol>
<li><p>TP(Tensor Parallel)：</p>
<p> 将 tensor 拆分运算，然后通过 <code>all-reduce</code> &#x2F; <code>all-gather</code> 汇总。例如 LLM 的 <code>Decoder</code> 采用多头注意力（multihead-attention），假设有 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.303ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 576 705" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="210E" xlink:href="#MJX-3-TEX-I-210E"></use></g></g></g></svg></mjx-container> 个头部，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 600 453" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-3-TEX-I-1D45B"></use></g></g></g></svg></mjx-container> 张卡，则可以每张卡 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="3.792ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1676 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path><path id="MJX-3-TEX-N-2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path><path id="MJX-3-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="210E" xlink:href="#MJX-3-TEX-I-210E"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(576,0)"><g data-mml-node="mo"><use data-c="2F" xlink:href="#MJX-3-TEX-N-2F"></use></g></g><g data-mml-node="mi" transform="translate(1076,0)"><use data-c="1D45B" xlink:href="#MJX-3-TEX-I-1D45B"></use></g></g></g></svg></mjx-container> 个头部并行计算。</p>
<p> <img src="/2025/04/28/vllm/multihead_attn.png"></p>
<pre><code class="python"># llama前向传播的代码
def forward(
    self,
    positions: torch.Tensor,
    hidden_states: torch.Tensor,
) -&gt; torch.Tensor:
    qkv, _ = self.qkv_proj(hidden_states)
    q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
    q, k = self.rotary_emb(positions, q, k)
    attn_output = self.attn(q, k, v)
    output, _ = self.o_proj(attn_output)
    return output
</code></pre>
</li>
<li><p>PP(Pipeline Parallel)：</p>
<p> 一个 request 由多个 node 流水线式地执行。可以提高吞吐，但是无法降低 <code>TTFT</code>。</p>
<p> 每个worker负责一个layers的子集</p>
<ul>
<li><code>vllm/model_executor/models/llama.py</code> 中 self.start_layer –&gt; self.end_layer   </li>
<li>在 worker 之间: <code>communicate IntermediateTensor</code></li>
<li><code>vllm/worker/model_runner.py</code>: 搜索 <code>get_pp_group()</code></li>
</ul>
</li>
<li><p>EP(Expert Parallel)：</p>
<p> 用于 MoE（Mixture of Expert）架构。在处理一个 request 的时候，只有一个很小的专家子集会被激活（通过门控控制）。router 会将 request 转发到相应的 expert 节点（MoE 的前置计算可能不是在相应的 expert 节点上进行的）。这里多个 expert 是并行的，同时计算不同的 batch。</p>
<ul>
<li>Shuffle (DeepEP communication kernel)</li>
<li>Forward</li>
<li>Shuffle back</li>
</ul>
</li>
</ol>
<p><img src="/2025/04/28/vllm/expert_parallel.png"></p>
<ol start="4">
<li><p>DP(Data Parallel)</p>
<p> 因为大型分布式生产环境中，EP &gt;&gt; TP（attention head 可能就十几个，而 experts 可能有几百个），这时候就需要 data parallel。</p>
<ul>
<li>TP * DP &#x3D;&#x3D; EP（通过请求并行的方式去拉满计算资源）</li>
<li>在实践中难以应用。</li>
<li>对请求进行padding避免造成死锁。</li>
</ul>
</li>
</ol>
<h3 id="Prefix-Caching"><a href="#Prefix-Caching" class="headerlink" title="Prefix Caching"></a>Prefix Caching</h3><p><img src="/2025/04/28/vllm/multi_turn_chat.png"></p>
<p>预备知识：</p>
<ol>
<li>LLM 的 <code>Prefill</code> 阶段：当用户输入一个 prompt（例如 “Hello, how are”），模型会一次性处理整个输入文本，计算所有输入 token 的隐藏状态。这个阶段通常是并行计算的，效率较高；LLM 的 <code>Decode</code> 阶段：模型从第一个生成的 token 开始，逐个预测后续的 token（例如 “you”、”today” 等）。每个新 token 的生成都依赖于前一个 token。</li>
<li>在多轮对话中，需要传递上下文，这些上下文总是共享一个前缀，如上图所示。显然，我们可以缓存这些前缀对应的 KV tensor，这也就是 <code>Prefix Caching</code>，缓存的对象便是 <code>K</code> 和 <code>V</code> tensor。</li>
<li><code>Prefix Caching</code> 只节省了 <code>Prefill</code> 阶段的耗时（也就是降低了 <code>TTFT</code> ，<code>Time To First Token</code>），并不能节省解码阶段的耗时（也就是 <code>TPOT</code> ，<code>Time Per Output Token</code>）。</li>
</ol>
<p><img src="/2025/04/28/vllm/prefix_caching.png"></p>
<p>vllm 中的 <code>Prefix Caching</code> 主要思路：</p>
<ol>
<li>将若干个 <code>tokens</code>（默认 16 个）组成一个 <code>block</code>，对于每个 <code>block</code>，计算其 hash 值并更新 <code>prefix hash</code>。</li>
<li>将 <code>prefix hash</code> -&gt; <code>kv cache</code> 的映射存储到 <code>kv store</code> 中（例如 redis，注意后者的 <code>kv</code> 指的是键值存储）。</li>
<li>若无法 <code>allocate</code> 缓存空间，以一定的策略（如 LFU，LRU）evict 一些缓存。</li>
</ol>
<p>伪代码：</p>
<pre><code class="python">prefix_hash = &quot;&quot;
for chunk in chunked_token:
    chunk_hash = hash(prefix_hash + chunk)
    prefix_hash = chunk_hash

for chunk_hash, chunk_kv in zip(...):
    kv_store.put(chunk_hash, chunk_kv)
</code></pre>
<h4 id="Prefix-Caching-的扩展——CacheBalend"><a href="#Prefix-Caching-的扩展——CacheBalend" class="headerlink" title="Prefix Caching 的扩展——CacheBalend"></a>Prefix Caching 的扩展——CacheBalend</h4><p><img src="/2025/04/28/vllm/cache_blend_1.png"></p>
<ul>
<li>Prefix Caching 只利用了前缀，缓存利用率有限；</li>
<li>Full KV Cache 使用所有的 Cache，会忽视 <code>cross attention</code>，产生低质量结果；</li>
<li>CacheBlend 通过重新计算一部分 KV，进行折中。</li>
</ul>
<p>以下是忽视 <code>cross attention</code> 的结果：</p>
<div style="text-align: center;">
  <img src="/2025/04/28/vllm/cache_blend_2.png" alt="Alt text" style="display: inline-block; margin-right: 10px;">
  <img src="/2025/04/28/vllm/cache_blend_3.png" alt="Alt text" style="display: inline-block;">
</div>

<h3 id="PD-分离"><a href="#PD-分离" class="headerlink" title="PD 分离"></a>PD 分离</h3><p>为什么需要 PD 分离？</p>
<blockquote>
<p>在大模型推理中，常用以下两项指标评估性能：<br>TTFT（Time-To-First-Token）：首 token 的生成时间，主要衡量 Prefill 阶段性能。<br>TPOT（Time-Per-Output-Token）：生成每个 token 的时间，主要衡量 Decode 阶段性能。<br>当 Prefill 和 Decode 在同一块 GPU 上运行时，由于两阶段的计算特性差异（Prefill 是计算密集型，而 Decode 是存储密集型），资源争抢会导致 TTFT 和 TPOT 之间的权衡。例如：</p>
<p>若优先处理 Prefill 阶段以降低 TTFT，Decode 阶段的性能（TPOT）可能下降。<br>若尽量提升 TPOT，则会增加 Prefill 请求的等待时间，导致 TTFT 上升。<br>PD 分离式架构的提出正是为了打破这一矛盾。通过将 Prefill 和 Decode 分离运行，可以针对不同阶段的特性独立优化资源分配，从而在降低首 token 延迟的同时提高整体吞吐量。</p>
</blockquote>
<p>Prefill 和 Decode 阶段分别受限于什么？</p>
<blockquote>
<p>Prefill 阶段：吞吐量随 batch size 增加逐渐趋于平稳。这是因为 Prefill 的计算受限特性（compute-bound），当 batch 中的总 token 数超过某个阈值时，计算资源成为瓶颈。</p>
<p>Decode 阶段：吞吐量随 batch size 增加显著提升。由于 Decode 阶段的存储受限特性（memory-bound），增大 batch size 可提高计算效率，从而显著增加吞吐量。</p>
</blockquote>
<p><img src="/2025/04/28/vllm/phase.png"></p>
<p>摘自 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/9433793184">LLM推理优化 - Prefill-Decode分离式推理架构</a></p>
<p><img src="/2025/04/28/vllm/pd_disaggregation.png"></p>
<ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/14689463165">Chunked Prefill</a></li>
<li>xP yD 问题，Prefill Instance 和 Decode Instance 的数量如何调整？</li>
<li>先 D 后 P 的模式：D node收到后先检查是否有 KVCache，没有的话再转给 P node去做，这个思路主要考虑的是 <code>TTFT</code>。</li>
</ol>
<h4 id="传递-KV-Cache"><a href="#传递-KV-Cache" class="headerlink" title="传递 KV Cache"></a>传递 KV Cache</h4><ul>
<li><p>两种模式：pooling 模式，P2P 模式，LMCache都支持上面两种模式，Mooncake(pooling)，NIXL(p2p)。</p>
</li>
<li><p>怎么从 vllm 提取（注入）KVCache</p>
<ul>
<li><code>connector API</code> in <code>vllm/worker/model_runner.py</code>。</li>
<li>在模型 <code>forward</code> 前：尝试接收 KVCache 并注入到到 vllm 的 pages memory 中。</li>
<li>在模型 <code>forward</code> 后，将 KVCache 从 pages memory 中并将它发送出去。</li>
</ul>
<p>  <img src="/2025/04/28/vllm/vllm_connector.png"></p>
</li>
<li><p><code>vllm/distributed/kv_transfer/kv_transfer_agent.py</code></p>
<pre><code class="python"># 本质上是根据 model input 计算出 KVCache 放在 page memory中的什么地方
def recv_kv_caches_and_hidden_states(
    self, model_executable: torch.nn.Module,
    model_input: &quot;ModelInputForGPUWithSamplingMetadata&quot;,
    kv_caches: List[torch.Tensor]
) -&gt; Tuple[Union[torch.Tensor, IntermediateTensors], bool,
        &quot;ModelInputForGPUWithSamplingMetadata&quot;]:
            
    return self.connector.recv_kv_caches_and_hidden_states(
        model_executable, model_input, kv_caches)
</code></pre>
<p>  可以先看 <code>vllm/distributed/kv_transfer/kv_connector/simple_connector.py</code>。</p>
</li>
</ul>
<h3 id="Speculative-Decoding"><a href="#Speculative-Decoding" class="headerlink" title="Speculative Decoding"></a>Speculative Decoding</h3><p>前文提到 Prefill 是 <code>gpu-bound</code>，计算密集型（把 request 的 tokens 全部输入 llm，生成第一个 token，并构建 KVCache）；而 Decode 是 <code>memory-bound</code>，依赖 Prefill 阶段生成的 KVCache，访存的时间往往大于计算的时间。那么有没有一种方法可以在不怎么增加 memory access 的前提下，提升计算的吞吐呢？<strong>有的，兄弟，有的</strong>，<code>Speculative Decoding</code>。</p>
<p><code>Speculative Decoding</code> 干了什么？其实就是去根据输入猜接下来的若干个 tokens 是什么，然后并行地进行验证。假如猜测 3 个 tokens，我们并行地验证这 3 个 tokens 是否正确的，由于是并行的，我们差不多只花费了原来只生成 1 个 token 的时间，最终获得了 3 个 tokens，也就是将吞吐提升了 3 倍，而访存只增加了（3 - 1）* token size。</p>
<p>那么如何猜呢？其实用一个古老的方法，<code>n-gram</code> 就行了。</p>
<blockquote>
<p>An n-gram is a sequence of n adjacent symbols in particular order. The symbols may be n adjacent letters (including punctuation marks and blanks), syllables, or rarely whole words found in a language dataset; or adjacent phonemes extracted from a speech-recording dataset, or adjacent base pairs extracted from a genome. They are collected from a text corpus or speech corpus.</p>
</blockquote>
<p><img src="/2025/04/28/vllm/ngram.png"></p>
<p>也就是说，我们根据一定的前缀，就能大致猜出后续的 token 搭配。<code>Speculative Decoding</code> 会根据输入构建 <code>ngram</code>，然后猜测后续的 tokens。</p>
<p><img src="/2025/04/28/vllm/speculative_decoding.png"></p>
<p>为什么这是有效的呢？下面几个 <code>workload</code> 就能说明这个问题：</p>
<ol>
<li>全文搜索，在用户给定的内容中寻找内容，或者给出一定答案。此处的回答一定是和用户内容强相关的，本质上会复读一部分；</li>
<li>代码生成场景，变量名、函数名等都很容易被 <code>ngram</code> 预测。</li>
</ol>
<h4 id="Tree-Verification"><a href="#Tree-Verification" class="headerlink" title="Tree Verification"></a>Tree Verification</h4><p><img src="/2025/04/28/vllm/tree_verification.png"></p>
<h4 id="Model-based（draft-model）Speculative-Decoding"><a href="#Model-based（draft-model）Speculative-Decoding" class="headerlink" title="Model-based（draft model）Speculative Decoding"></a>Model-based（draft model）Speculative Decoding</h4><ol>
<li><p>Parallel guessing（并行猜测）</p>
<ul>
<li>优点：快，在不知道第一个 token 情况下直接猜第二个。</li>
<li>缺点：在猜测第二个 token 的时候不知道第一个token是什么，容易胡言乱语</li>
</ul>
</li>
<li><p>Autoregression guessing（自回归猜测）</p>
<ul>
<li>优点：在猜测第二个 token 的时候知道第一个 token，准确率较高</li>
<li>缺点：慢</li>
</ul>
</li>
</ol>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/AI-Infra/" style="color: #00bcd4">AI Infra</a>
        </span>
        
        <span class="tag">
            
            <a href="/tags/LLM/" style="color: #00a596">LLM</a>
        </span>
        
        <span class="tag">
            
            <a href="/tags/vllm/" style="color: #ff7d73">vllm</a>
        </span>
        
    </div>
    <a href="/2025/04/28/vllm/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2025/04/26/rocksdb/">
        <h2 class="post-title">RocksDB 学习笔记</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/Database/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                Database
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/4/26
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            

	<div class="row">
    <embed src="/2025/04/26/rocksdb/rocksdb.pdf" width="100%" height="550" type="application/pdf">
	</div>




<h3 id="RocksDB-架构"><a href="#RocksDB-架构" class="headerlink" title="RocksDB 架构"></a>RocksDB 架构</h3><p><img src="/2025/04/26/rocksdb/arch.png"></p>
<p>先写入 <code>WAL</code>，再写入 <code>MemTable</code>。<code>MemTable</code> 达到一定的大小就会变为 <code>Immutable MemTable</code>，后台异步以 <code>SSTable</code> 的形式 flush 到磁盘中。</p>
<p><code>MemTable</code> 中还包含了一个 <code>Bloom Filter</code>，用于快速判断一个元素是否<strong>不存在</strong>。</p>
<h3 id="Column-Family"><a href="#Column-Family" class="headerlink" title="Column Family"></a>Column Family</h3><p>RocksDB 中列族是一个逻辑上的分组，列族之间使用不同的 LSM Tree，但是共享 WAL。</p>
<h3 id="Version"><a href="#Version" class="headerlink" title="Version"></a>Version</h3><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/coguin/p/11405082.html">RocksDB Version管理概述</a></p>
<p><img src="/2025/04/26/rocksdb/version.png"></p>
<p><code>Version</code> 会指向不同的 SST，并且 <code>Version</code> 和 <code>SSTable</code> 都是以引用计数的方式维护，一旦引用计数变为 0，就会进行回收。</p>
<h3 id="Write-Batch"><a href="#Write-Batch" class="headerlink" title="Write Batch"></a>Write Batch</h3><p>参考文献：</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://youjiali1995.github.io/rocksdb/write-batch/">RocksDB 源码分析 – Write Batch</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/643930273">RocksDB 剖析 | WriteThread 如何控制并发写入流程</a></li>
</ol>
<p>上面这篇文章讲的挺清楚的，RocksDB 默认是不开启并行写的，因为跳表结构并不适合并发写入，WAL 如果要并发写也需要一定同步机制维护其写入顺序。所以，RocksDB 采用的是分批写入的方式，以保证其吞吐量。<code>Write Batch Group</code> 会有一个 <code>leader</code>（当前的第一个 <code>writer</code>），然后后面的 <code>writer</code> 将成为 <code>follower</code>，构成一条双向链表（通过 <code>link_older</code>，<code>link_newer</code>）相连。这个 <code>group</code> 会有个大小上限，超过之后就会截断。截断后的新 <code>writer</code> 将会变为新的 <code>leader</code>，构成新的 <code>group</code>。</p>
<p>RocksDB 使用 <code>atomic::&lt;Writer *&gt;</code> 的形式组织链表，可以通过 <code>CAS</code> 以 <code>latch-free</code> 的方式添加 <code>writer</code>：</p>
<pre><code class="c++">Writer* writers = newest_writer-&gt;load(std::memory_order_relaxed);
while (true) &#123;
  w-&gt;link_older = writers;
  if (newest_writer-&gt;compare_exchange_weak(writers, w)) &#123;
    return (writers == nullptr);
  &#125;
&#125;
</code></pre>
<p>这边正好学习了一下 <code>std::atomic</code> 的用法，比起其他语言，c++ 的原子对象的 <code>load</code> 操作还可以选择不同的参数：</p>
<ol>
<li><code>std::memory_order_relaxed</code>：仅保证原子性，不强制内存顺序。</li>
<li><code>std::memory_order_acquire</code>：确保后续读操作不会重排到此操作之前。</li>
<li><code>std::memory_order_seq_cst</code>（默认）：顺序一致性，提供最强的内存屏障。</li>
</ol>
<p>尝试写了一个链表插入练习：</p>
<pre><code class="c++">#include &lt;atomic&gt;
#include &lt;iostream&gt;
#include &lt;thread&gt;

struct ListNode &#123;
  int val;

  std::atomic&lt;ListNode *&gt; next;
&#125;;

void insertListNode(std::atomic&lt;ListNode *&gt; &amp;tail, ListNode **root, int val) &#123;
  ListNode *newNode = new ListNode;
  newNode-&gt;val = val;
  newNode-&gt;next = nullptr;
  ListNode *oldTail = tail.load(std::memory_order_relaxed);
  while (true) &#123;
    if (tail.compare_exchange_weak(oldTail, newNode)) &#123;
      break;
    &#125;
  &#125;
  if (oldTail == nullptr) &#123;
    *root = newNode;
  &#125; else &#123;
    oldTail-&gt;next = newNode;
  &#125;
&#125;

void printList(ListNode *root) &#123;
  if (root == nullptr) &#123;
    std::cout &lt;&lt; &quot;List is empty&quot; &lt;&lt; std::endl;
    return;
  &#125;
  ListNode *p = root;
  while (p) &#123;
    std::cout &lt;&lt; p-&gt;val &lt;&lt; &quot; &quot;;
    p = p-&gt;next.load(std::memory_order_relaxed);
  &#125;
  std::cout &lt;&lt; std::endl;
&#125;

int main() &#123;
  std::atomic&lt;ListNode *&gt; tail = nullptr;
  ListNode *root = nullptr;
  for (int i = 0; i &lt; 3; i++) &#123;
    std::thread t([&amp;, i]() &#123;
      while (1) &#123;
        insertListNode(tail, &amp;root, i);
        std::this_thread::sleep_for(std::chrono::milliseconds(300));
      &#125;
    &#125;);
    t.detach();
  &#125;

  while (1) &#123;
    printList(root);
    std::this_thread::sleep_for(std::chrono::milliseconds(300));
  &#125;
&#125;
</code></pre>
<p>一种 Lazy Initialize 的方式：</p>
<pre><code class="c++">std::optional&lt;std::mutex&gt; mtx;
mtx.emplace();
</code></pre>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/Database/" style="color: #03a9f4">Database</a>
        </span>
        
    </div>
    <a href="/2025/04/26/rocksdb/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2025/04/20/foundation-db/">
        <h2 class="post-title">Foundation DB 学习笔记</h2>
    </a>
    <div class="category-and-date">
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/4/20
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            

	<div class="row">
    <embed src="/2025/04/20/foundation-db/paper.pdf" width="100%" height="550" type="application/pdf">
	</div>




<p>在线讲解：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1834y1Y7m7/?spm_id_from=333.1391.0.0&vd_source=f7e4c2acec163bdcd3e200e3623cc3e3">[Paper Reading] FoundationDB：开源分布式 KV 存储的内部实现原理</a></p>
<p>最近看 DeepSeek 的 3FS 的时候发现他们会用 FoundationDB 做 <code>chunk metadata</code> 的存储（3FS 有点像 GFS，也是以 chunk 作为存储单位）。正好不久之后要去阿里云开始一段存储相关的实习，学习一下相关知识。</p>
<p>FoundationDB 是一个分布式 KV 存储数据库，适用于读多写少的场景，支持 SSI（Serializable Snapshot Isolation）级别的事务。</p>
<h3 id="FoundationDB-如何实现-SSI"><a href="#FoundationDB-如何实现-SSI" class="headerlink" title="FoundationDB 如何实现 SSI"></a>FoundationDB 如何实现 SSI</h3><p>SSI 这一级别是通过 OCC + MVCC 实现的。</p>
<p>首先，让我们复习一下事务隔离级别，可以从这篇文章切入：<a target="_blank" rel="noopener" href="https://lotabout.me/2020/QQA-Isolation-Level-of-Database/">事务隔离级别备忘</a>。通过 MVCC 的快照，我们可以很容易地实现 Snapshot Isolation，虽然这解决了脏读和幻读的问题，但是无法解决两种问题（也就是上面链接中的 P4 和 A5B 问题）。</p>
<ul>
<li><p>P4 Lost Update</p>
<p>  <img src="/2025/04/20/foundation-db/lost_update.png"></p>
<p>  T2 修改 x 为 120，然而 T2 提交后被 T1 提交的修改覆盖，最终 x &#x3D; 130。对外界而言，x &#x3D; 120 这一修改不可见，被覆盖。</p>
</li>
<li><p>A5B Write Skew</p>
<p>  <img src="/2025/04/20/foundation-db/write_skew.png"></p>
<p>  x + y 应该满足约束 <code>x + y &lt;= 100</code>，由于 T1 和 T2 的 <code>write set</code> 没有相交，所以两个事务都能提交成功，然而约束已经被破坏。</p>
</li>
</ul>
<p>为了解决上述问题，FoundationDB 会维护一个 <code>read set</code>，如果 <code>read set</code> 中的某个 <code>key range</code> 对应的最新的一次 <code>commit version</code> 大于事务的 <code>read version</code>，就会 abort 当前事务。这种乐观的方式有若干好处：</p>
<ul>
<li>无需锁，高效；</li>
<li>读多写少，abort 是小概率事件。</li>
</ul>
<p>在 FoundationDB 中，<code>resolvers</code> 会保留最近一段时间（如 5 秒，MVCC window）内已提交的写入操作在内存中，这个时间范围就可以看作是一个 “窗口”。当一个新的事务进行冲突检测时，会将其读取的数据与这个窗口内的写入操作进行比较，以确定是否存在数据冲突。如果在这个窗口内，事务读取的数据被其他事务修改了，那么就会检测到冲突，该事务可能会被中止或重试。</p>
<p><img src="/2025/04/20/foundation-db/example.png"></p>
<h3 id="FoundationDB-的架构"><a href="#FoundationDB-的架构" class="headerlink" title="FoundationDB 的架构"></a>FoundationDB 的架构</h3><p><img src="/2025/04/20/foundation-db/architecture.png"></p>
<p>整个系统是自上而下启动的。FoundationDB 的控制面是通过 Paxos 算法选举出 Cluster Controller（即 leader），如下图所示。</p>
<p><img src="/2025/04/20/foundation-db/control_plane.png"></p>
<p>下图为数据面，主要由 TS + LS + SS 组成。</p>
<p><img src="/2025/04/20/foundation-db/data_plane.png"></p>
<h4 id="TS"><a href="#TS" class="headerlink" title="TS"></a>TS</h4><ul>
<li>Sequencer：<ul>
<li>负责获取 read&#x2F;commit version。</li>
</ul>
</li>
<li>Proxy：<ul>
<li>一个中介，负责获取 read version 和事务提交。</li>
</ul>
</li>
<li>Resolver：<ul>
<li>分布式地解决读写冲突。</li>
<li>多节点，按照 <code>key range</code> 分片（shard）。</li>
</ul>
</li>
</ul>
<h4 id="LS"><a href="#LS" class="headerlink" title="LS"></a>LS</h4><ul>
<li>按照 <code>key range</code> 分片（shard）。</li>
</ul>
<h4 id="SS"><a href="#SS" class="headerlink" title="SS"></a>SS</h4><ul>
<li>单机存储系统，从 LS 的 WAL 异步复制数据。</li>
<li>多版本数据只存储在内存中</li>
<li>5s 的 MVCC window（太长了会产生大量假阳性）</li>
</ul>
<h3 id="Log-System"><a href="#Log-System" class="headerlink" title="Log System"></a>Log System</h3><ul>
<li>Log Server 下面绑了若干 Storage Server。</li>
<li>按照 key range 映射到若干个 replica。</li>
<li>之所以需要向未被映射到的 Log Server 传递空消息，是因为 Log Server 是按照顺序执行的，不这样做会产生“空洞”。</li>
</ul>
<p><img src="/2025/04/20/foundation-db/log_system.png"></p>
<h3 id="Commit-Path"><a href="#Commit-Path" class="headerlink" title="Commit Path"></a>Commit Path</h3><ol>
<li>Client:<ol>
<li>Buffer Modifications</li>
<li>Send Read&#x2F;Write Set To Proxy</li>
</ol>
</li>
<li>Proxy: Get Commit Version from Sequencer</li>
<li>Proxy: Send User Data KeyRange To Resolvers</li>
<li>Resolver: Check RW Conflict</li>
<li>Proxy: Commit Phase When no Conflict<ol>
<li>Send Data to LS</li>
<li>Committed When All LS return Success</li>
<li>Send Commit Version to Sequencer for Read</li>
<li>Proxy return OK to Client</li>
</ol>
</li>
<li>SS pull WAL from LS</li>
</ol>
<p>Read: 1 roundtrip<br>Write: 4 roundtrip</p>
<h3 id="Recovery-Replication"><a href="#Recovery-Replication" class="headerlink" title="Recovery &amp;&amp; Replication"></a>Recovery &amp;&amp; Replication</h3><p><img src="/2025/04/20/foundation-db/recovery.png"></p>
<p>k &#x3D; f + 1</p>
<p>这里 k 表示需要复制的日志份数，f 表示系统可容忍的故障节点数。即需要复制的日志份数比可容忍的故障节点数多 1，通过这种方式来保证即使部分节点故障，日志信息也不会丢失，能维持系统的正常运行和数据一致性。</p>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/Database/" style="color: #ffa2c4">Database</a>
        </span>
        
        <span class="tag">
            
            <a href="/tags/Key-Value-Store/" style="color: #ff7d73">Key-Value Store</a>
        </span>
        
    </div>
    <a href="/2025/04/20/foundation-db/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2025/03/24/sfinae/">
        <h2 class="post-title">SFINAE —— Substitution Failure Is Not An Error</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/Language/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                Language
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/3/24
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h3 id="IsConvertible"><a href="#IsConvertible" class="headerlink" title="IsConvertible"></a>IsConvertible</h3><pre><code class="c++">#include &lt;iostream&gt;
#include &lt;string&gt;

template &lt;typename FROM, typename TO&gt; struct IsConvetible &#123;
  // FROM should be converted to TO implicitly
  static void aux(TO);

  // Should use another typename F, otherwise it will cause an compilation error
  // immediately
  template &lt;typename F, typename = decltype(aux(std::declval&lt;F&gt;()))&gt;
  static std::true_type test(void *);

  // Will failed with: No viable conversion from &#39;double&#39; to &#39;std::string&#39;
  //   template &lt;typename = decltype(aux(std::declval&lt;FROM&gt;()))&gt;
  //   static std::true_type test(void *);

  // If the above function failed, it will fallback here
  template &lt;typename&gt; static std::false_type test(...);

  static constexpr bool value = decltype(test&lt;FROM&gt;(nullptr))::value;
&#125;;

int main() &#123;
  std::cout &lt;&lt; std::boolalpha;
  std::cout &lt;&lt; IsConvetible&lt;double, std::string&gt;::value &lt;&lt; std::endl;
  std::cout &lt;&lt; IsConvetible&lt;double, int&gt;::value &lt;&lt; std::endl;
  std::cout &lt;&lt; IsConvetible&lt;const char *, std::string&gt;::value &lt;&lt; std::endl;
  std::cout &lt;&lt; IsConvetible&lt;const char[10], std::string&gt;::value &lt;&lt; std::endl;
  std::cout &lt;&lt; IsConvetible&lt;char[10], std::string&gt;::value &lt;&lt; std::endl;
&#125;
</code></pre>
<p>输出：</p>
<pre><code>false
true
true
true
true
</code></pre>
<h3 id="IsDefaultConstructible"><a href="#IsDefaultConstructible" class="headerlink" title="IsDefaultConstructible"></a>IsDefaultConstructible</h3><pre><code class="c++">#include &lt;iostream&gt;
#include &lt;string&gt;

template &lt;typename T&gt; struct IsDefaultConstructibleHelper &#123;

  // Should use another typename U, otherwise it will cause an compilation error
  // immediately
  template &lt;typename U, typename = decltype(U())&gt;
  static std::true_type test(void *);

  // If the above function failed, it will fallback here.
  template &lt;typename&gt; static std::false_type test(...);

  static constexpr bool value = decltype(test&lt;T&gt;(nullptr))::value;
&#125;;

template &lt;typename T&gt;
constexpr bool IsDefaultConstructible = IsDefaultConstructibleHelper&lt;T&gt;::value;

class WithDefaultConstructor &#123;&#125;;

class WithoutDefaultConstructor &#123;
  WithoutDefaultConstructor() = delete;
&#125;;

int main() &#123;
  std::cout &lt;&lt; std::boolalpha;
  std::cout &lt;&lt; IsDefaultConstructible&lt;int&gt; &lt;&lt; std::endl;
  std::cout &lt;&lt; IsDefaultConstructible&lt;WithDefaultConstructor&gt; &lt;&lt; std::endl;
  std::cout &lt;&lt; IsDefaultConstructible&lt;WithoutDefaultConstructor&gt; &lt;&lt; std::endl;
&#125;
</code></pre>
<p>输出：</p>
<pre><code>true
true
false
</code></pre>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/C/" style="color: #ff7d73">C++</a>
        </span>
        
    </div>
    <a href="/2025/03/24/sfinae/" class="go-post">阅读全文</a>
</div>


        <div class="page-current">
    
    <span class="current">1</span>
    
    <a class="page-num" href="/page/2">2</a>
    
    
    <a class="page-num" href="/page/3">3</a>
    
    
    <span class="page-omit">...</span>
    <a class="page-num" href="/page/4">4</a>
    
    
    <a class="page-num" href="/page/2/">
        <i class="fa-solid fa-caret-right fa-fw"></i>
    </a>
    
</div>

    </div>
    
    <div id="home-card">
        <div id="card-style">
    <div id="card-div">
        <div class="avatar">
            <img src="/images/avatar.jpg" alt="avatar" />
        </div>
        <div class="name">Zihong Lin</div>
        <div class="description">
            <p>What I can’t create, I don’t understand.</p>

        </div>
        
        
    </div>
</div>

    </div>
    
</div>

            <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2022 - 2025 Okabe&#39;s LAB
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;Zihong Lin
        </div>
        <div>
            Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp;
            <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a>
        </div>
        
    </div>
</footer>

        </div>
        
        <transition name="fade">
            <div id="preview" ref="preview" v-show="previewShow">
                <img id="preview-content" ref="previewContent" />
            </div>
        </transition>
        
    </div>
    <script src="/js/main.js"></script>
    
</body>
</html>
