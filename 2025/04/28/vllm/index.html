
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <title>vllm 学习笔记 | Okabe&#39;s LAB</title>
    <meta name="author" content="Zihong Lin" />
    <meta name="description" content="" />
    <meta name="keywords" content="" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <link rel="icon" href="/images/avatar.jpg" />
    <link rel="preconnect" href="https://cdn.staticfile.org" />
<script src="https://cdn.staticfile.org/vue/3.3.7/vue.global.prod.min.js"></script>
<link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/6.4.2/css/all.min.css" />
<link rel="preconnect" href="https://fonts.loli.net" />
<link rel="preconnect" href="https://gstatic.loli.net" crossorigin />
<link rel="stylesheet" href="https://fonts.loli.net/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap" />
<script> const mixins = {}; </script>

<script src="https://polyfill.io/v3/polyfill.min.js?features=default"></script>


<script src="https://cdn.staticfile.org/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.org/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script>
<link
    rel="stylesheet"
    href="https://cdn.staticfile.org/highlight.js/11.9.0/styles/github.min.css"
/>
<script src="/js/lib/highlight.js"></script>



<script src="/js/lib/preview.js"></script>









<link rel="stylesheet" href="/css/main.css" />

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.1.1"></head>
<body>
    <div id="layout">
        <transition name="fade">
            <div id="loading" v-show="loading">
                <div id="loading-circle">
                    <h2>LOADING</h2>
                    <p>加载过慢请开启缓存 浏览器默认开启</p>
                    <img src="/images/loading.gif" />
                </div>
            </div>
        </transition>
        <div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}">
    <nav id="desktop-menu">
        <a class="title" href="/">
            <span>OKABE&#39;S LAB</span>
        </a>
        
        <a href="/">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;Home</span>
        </a>
        
        <a href="/about">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;About</span>
        </a>
        
        <a href="/archives">
            <i class="fa-solid fa-box-archive fa-fw"></i>
            <span>&ensp;Archives</span>
        </a>
        
        <a href="/categories">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;Categories</span>
        </a>
        
        <a href="/tags">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;Tags</span>
        </a>
        
    </nav>
    <nav id="mobile-menu">
        <div class="title" @click="showMenuItems = !showMenuItems">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;OKABE&#39;S LAB</span>
        </div>
        <transition name="slide">
            <div class="items" v-show="showMenuItems">
                
                <a href="/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-house fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Home</div>
                    </div>
                </a>
                
                <a href="/about">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-id-card fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">About</div>
                    </div>
                </a>
                
                <a href="/archives">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-box-archive fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Archives</div>
                    </div>
                </a>
                
                <a href="/categories">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-bookmark fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Categories</div>
                    </div>
                </a>
                
                <a href="/tags">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-tags fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Tags</div>
                    </div>
                </a>
                
            </div>
        </transition>
    </nav>
</div>
<transition name="fade">
    <div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div>
</transition>

        <div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'">
            <div class="article">
    <div>
        <h1>vllm 学习笔记</h1>
    </div>
    <div class="info">
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/4/28
        </span>
        
        <span class="category">
            <a href="/categories/AI/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                AI
            </a>
        </span>
        
        
        <span class="tags">
            <span class="icon">
                <i class="fa-solid fa-tags fa-fw"></i>
            </span>
            
            
            <span class="tag">
                
                <a href="/tags/LLM/" style="color: #00a596">LLM</a>
            </span>
            
            <span class="tag">
                
                <a href="/tags/vllm/" style="color: #ffa2c4">vllm</a>
            </span>
            
            <span class="tag">
                
                <a href="/tags/AI-Infra/" style="color: #ff7d73">AI Infra</a>
            </span>
            
        </span>
        
    </div>
    
    <div class="content" v-pre>
        <h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/717581669">大模型推理加速与KV Cache（一）：什么是KV Cache</a></li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=W83Zgbg8SkE&list=PLJj_urhaf2_qxpg8A5-6xoMvMLBKQMTX1&index=9">[FIXME][EP02][直播版] vllm源码讲解，分布式推理</a></li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=mWvqA_BNtsU&list=PLJj_urhaf2_qxpg8A5-6xoMvMLBKQMTX1">[FIXME][EP05][直播版] vllm从开源到部署，Prefix Caching和开源答疑</a></li>
<li><a target="_blank" rel="noopener" href="https://kevincheung2259.github.io/2025/03/29/CacheBlend/">CacheBlend-高效提高KVCache复用性的方法</a></li>
</ol>
<h3 id="Distributed-Inference"><a href="#Distributed-Inference" class="headerlink" title="Distributed Inference"></a>Distributed Inference</h3><p>具体的通信代码可以参考：<a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/llama.py">vllm&#x2F;model_executor&#x2F;models&#x2F;llama.py</a></p>
<h4 id="分布式通信手段"><a href="#分布式通信手段" class="headerlink" title="分布式通信手段"></a>分布式通信手段</h4><ol>
<li>nvlink</li>
<li>InfiniBand</li>
<li>RDMA</li>
</ol>
<h4 id="并行方式"><a href="#并行方式" class="headerlink" title="并行方式"></a>并行方式</h4><ol>
<li><p>TP(Tensor Parallel)：</p>
<p> 将 tensor 拆分运算，然后通过 <code>all-reduce</code> &#x2F; <code>all-gather</code> 汇总。例如 LLM 的 <code>Decoder</code> 采用多头注意力（multihead-attention），假设有 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.303ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 576 705" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="210E" xlink:href="#MJX-1-TEX-I-210E"></use></g></g></g></svg></mjx-container> 个头部，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 600 453" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-1-TEX-I-1D45B"></use></g></g></g></svg></mjx-container> 张卡，则可以每张卡 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="3.792ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1676 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path><path id="MJX-1-TEX-N-2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path><path id="MJX-1-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="210E" xlink:href="#MJX-1-TEX-I-210E"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(576,0)"><g data-mml-node="mo"><use data-c="2F" xlink:href="#MJX-1-TEX-N-2F"></use></g></g><g data-mml-node="mi" transform="translate(1076,0)"><use data-c="1D45B" xlink:href="#MJX-1-TEX-I-1D45B"></use></g></g></g></svg></mjx-container> 个头部并行计算。</p>
<p> <img src="/2025/04/28/vllm/multihead_attn.png"></p>
<pre><code class="python"># llama前向传播的代码
def forward(
    self,
    positions: torch.Tensor,
    hidden_states: torch.Tensor,
) -&gt; torch.Tensor:
    qkv, _ = self.qkv_proj(hidden_states)
    q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
    q, k = self.rotary_emb(positions, q, k)
    attn_output = self.attn(q, k, v)
    output, _ = self.o_proj(attn_output)
    return output
</code></pre>
</li>
<li><p>PP(Pipeline Parallel)：</p>
<p> 一个 request 由多个 node 流水线式地执行。可以提高吞吐，但是无法降低 <code>TTFT</code>。</p>
<p> 每个worker负责一个layers的子集</p>
<ul>
<li><code>vllm/model_executor/models/llama.py</code> 中 self.start_layer –&gt; self.end_layer   </li>
<li>在 worker 之间: <code>communicate IntermediateTensor</code></li>
<li><code>vllm/worker/model_runner.py</code>: 搜索 <code>get_pp_group()</code></li>
</ul>
</li>
<li><p>EP(Expert Parallel)：</p>
<p> 用于 MoE（Mixture of Expert）架构。在处理一个 request 的时候，只有一个很小的专家子集会被激活（通过门控控制）。router 会将 request 转发到相应的 expert 节点（MoE 的前置计算可能不是在相应的 expert 节点上进行的）。这里多个 expert 是并行的，同时计算不同的 batch。</p>
<ul>
<li>Shuffle (DeepEP communication kernel)</li>
<li>Forward</li>
<li>Shuffle back</li>
</ul>
</li>
</ol>
<p><img src="/2025/04/28/vllm/expert_parallel.png"></p>
<ol start="4">
<li><p>DP(Data Parallel)</p>
<p> 因为大型分布式生产环境中，EP &gt;&gt; TP（attention head 可能就十几个，而 experts 可能有几百个），这时候就需要 data parallel。</p>
<ul>
<li>TP * DP &#x3D;&#x3D; EP（通过请求并行的方式去拉满计算资源）</li>
<li>在实践中难以应用。</li>
<li>对请求进行padding避免造成死锁。</li>
</ul>
</li>
</ol>
<h3 id="Prefix-Caching"><a href="#Prefix-Caching" class="headerlink" title="Prefix Caching"></a>Prefix Caching</h3><p><img src="/2025/04/28/vllm/multi_turn_chat.png"></p>
<p>预备知识：</p>
<ol>
<li>LLM 的 <code>Prefill</code> 阶段：当用户输入一个 prompt（例如 “Hello, how are”），模型会一次性处理整个输入文本，计算所有输入 token 的隐藏状态。这个阶段通常是并行计算的，效率较高；LLM 的 <code>Decode</code> 阶段：模型从第一个生成的 token 开始，逐个预测后续的 token（例如 “you”、”today” 等）。每个新 token 的生成都依赖于前一个 token。</li>
<li>在多轮对话中，需要传递上下文，这些上下文总是共享一个前缀，如上图所示。显然，我们可以缓存这些前缀对应的 KV tensor，这也就是 <code>Prefix Caching</code>，缓存的对象便是 <code>K</code> 和 <code>V</code> tensor。</li>
<li><code>Prefix Caching</code> 只节省了 <code>Prefill</code> 阶段的耗时（也就是降低了 <code>TTFT</code> ，<code>Time To First Token</code>），并不能节省解码阶段的耗时（也就是 <code>TPOT</code> ，<code>Time Per Output Token</code>）。</li>
</ol>
<p><img src="/2025/04/28/vllm/prefix_caching.png"></p>
<p>vllm 中的 <code>Prefix Caching</code> 主要思路：</p>
<ol>
<li>将若干个 <code>tokens</code>（默认 16 个）组成一个 <code>block</code>，对于每个 <code>block</code>，计算其 hash 值并更新 <code>prefix hash</code>。</li>
<li>将 <code>prefix hash</code> -&gt; <code>kv cache</code> 的映射存储到 <code>kv store</code> 中（例如 redis，注意后者的 <code>kv</code> 指的是键值存储）。</li>
<li>若无法 <code>allocate</code> 缓存空间，以一定的策略（如 LFU，LRU）evict 一些缓存。</li>
</ol>
<p>伪代码：</p>
<pre><code class="python">prefix_hash = &quot;&quot;
for chunk in chunked_token:
    chunk_hash = hash(prefix_hash + chunk)
    prefix_hash = chunk_hash

for chunk_hash, chunk_kv in zip(...):
    kv_store.put(chunk_hash, chunk_kv)
</code></pre>
<h4 id="Prefix-Caching-的扩展——CacheBalend"><a href="#Prefix-Caching-的扩展——CacheBalend" class="headerlink" title="Prefix Caching 的扩展——CacheBalend"></a>Prefix Caching 的扩展——CacheBalend</h4><p><img src="/2025/04/28/vllm/cache_blend_1.png"></p>
<ul>
<li>Prefix Caching 只利用了前缀，缓存利用率有限；</li>
<li>Full KV Cache 使用所有的 Cache，会忽视 <code>cross attention</code>，产生低质量结果；</li>
<li>CacheBlend 通过重新计算一部分 KV，进行折中。</li>
</ul>
<p>以下是忽视 <code>cross attention</code> 的结果：</p>
<div style="text-align: center;">
  <img src="/2025/04/28/vllm/cache_blend_2.png" alt="Alt text" style="display: inline-block; margin-right: 10px;">
  <img src="/2025/04/28/vllm/cache_blend_3.png" alt="Alt text" style="display: inline-block;">
</div>

<h3 id="PD-分离"><a href="#PD-分离" class="headerlink" title="PD 分离"></a>PD 分离</h3><p>为什么需要 PD 分离？</p>
<blockquote>
<p>在大模型推理中，常用以下两项指标评估性能：<br>TTFT（Time-To-First-Token）：首 token 的生成时间，主要衡量 Prefill 阶段性能。<br>TPOT（Time-Per-Output-Token）：生成每个 token 的时间，主要衡量 Decode 阶段性能。<br>当 Prefill 和 Decode 在同一块 GPU 上运行时，由于两阶段的计算特性差异（Prefill 是计算密集型，而 Decode 是存储密集型），资源争抢会导致 TTFT 和 TPOT 之间的权衡。例如：</p>
<p>若优先处理 Prefill 阶段以降低 TTFT，Decode 阶段的性能（TPOT）可能下降。<br>若尽量提升 TPOT，则会增加 Prefill 请求的等待时间，导致 TTFT 上升。<br>PD 分离式架构的提出正是为了打破这一矛盾。通过将 Prefill 和 Decode 分离运行，可以针对不同阶段的特性独立优化资源分配，从而在降低首 token 延迟的同时提高整体吞吐量。</p>
</blockquote>
<p>Prefill 和 Decode 阶段分别受限于什么？</p>
<blockquote>
<p>Prefill 阶段：吞吐量随 batch size 增加逐渐趋于平稳。这是因为 Prefill 的计算受限特性（compute-bound），当 batch 中的总 token 数超过某个阈值时，计算资源成为瓶颈。</p>
<p>Decode 阶段：吞吐量随 batch size 增加显著提升。由于 Decode 阶段的存储受限特性（memory-bound），增大 batch size 可提高计算效率，从而显著增加吞吐量。</p>
</blockquote>
<p><img src="/2025/04/28/vllm/phase.png"></p>
<p>摘自 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/9433793184">LLM推理优化 - Prefill-Decode分离式推理架构</a></p>
<p><img src="/2025/04/28/vllm/pd_disaggregation.png"></p>
<ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/14689463165">Chunked Prefill</a></li>
<li>xP yD 问题，Prefill Instance 和 Decode Instance 的数量如何调整？</li>
<li>先 D 后 P 的模式：D node收到后先检查是否有 KVCache，没有的话再转给 P node去做，这个思路主要考虑的是 <code>TTFT</code>。</li>
</ol>
<h4 id="传递-KV-Cache"><a href="#传递-KV-Cache" class="headerlink" title="传递 KV Cache"></a>传递 KV Cache</h4><ul>
<li><p>两种模式：pooling 模式，P2P 模式，LMCache都支持上面两种模式，Mooncake(pooling)，NIXL(p2p)。</p>
</li>
<li><p>怎么从 vllm 提取（注入）KVCache</p>
<ul>
<li><code>connector API</code> in <code>vllm/worker/model_runner.py</code>。</li>
<li>在模型 <code>forward</code> 前：尝试接收 KVCache 并注入到到 vllm 的 pages memory 中。</li>
<li>在模型 <code>forward</code> 后，将 KVCache 从 pages memory 中并将它发送出去。</li>
</ul>
<p>  <img src="/2025/04/28/vllm/vllm_connector.png"></p>
</li>
<li><p><code>vllm/distributed/kv_transfer/kv_transfer_agent.py</code></p>
<pre><code class="python"># 本质上是根据 model input 计算出 KVCache 放在 page memory中的什么地方
def recv_kv_caches_and_hidden_states(
    self, model_executable: torch.nn.Module,
    model_input: &quot;ModelInputForGPUWithSamplingMetadata&quot;,
    kv_caches: List[torch.Tensor]
) -&gt; Tuple[Union[torch.Tensor, IntermediateTensors], bool,
        &quot;ModelInputForGPUWithSamplingMetadata&quot;]:
            
    return self.connector.recv_kv_caches_and_hidden_states(
        model_executable, model_input, kv_caches)
</code></pre>
<p>  可以先看 <code>vllm/distributed/kv_transfer/kv_connector/simple_connector.py</code>。</p>
</li>
</ul>
<h3 id="Speculative-Decoding"><a href="#Speculative-Decoding" class="headerlink" title="Speculative Decoding"></a>Speculative Decoding</h3><p>前文提到 Prefill 是 <code>gpu-bound</code>，计算密集型（把 request 的 tokens 全部输入 llm，生成第一个 token，并构建 KVCache）；而 Decode 是 <code>memory-bound</code>，依赖 Prefill 阶段生成的 KVCache，访存的时间往往大于计算的时间。那么有没有一种方法可以在不怎么增加 memory access 的前提下，提升计算的吞吐呢？<strong>有的，兄弟，有的</strong>，<code>Speculative Decoding</code>。</p>
<p><code>Speculative Decoding</code> 干了什么？其实就是去根据输入猜接下来的若干个 tokens 是什么，然后并行地进行验证。假如猜测 3 个 tokens，我们并行地验证这 3 个 tokens 是否正确的，由于是并行的，我们差不多只花费了原来只生成 1 个 token 的时间，最终获得了 3 个 tokens，也就是将吞吐提升了 3 倍，而访存只增加了（3 - 1）* token size。</p>
<p>那么如何猜呢？其实用一个古老的方法，<code>n-gram</code> 就行了。</p>
<blockquote>
<p>An n-gram is a sequence of n adjacent symbols in particular order. The symbols may be n adjacent letters (including punctuation marks and blanks), syllables, or rarely whole words found in a language dataset; or adjacent phonemes extracted from a speech-recording dataset, or adjacent base pairs extracted from a genome. They are collected from a text corpus or speech corpus.</p>
</blockquote>
<p><img src="/2025/04/28/vllm/ngram.png"></p>
<p>也就是说，我们根据一定的前缀，就能大致猜出后续的 token 搭配。<code>Speculative Decoding</code> 会根据输入构建 <code>ngram</code>，然后猜测后续的 tokens。</p>
<p><img src="/2025/04/28/vllm/speculative_decoding.png"></p>
<p>为什么这是有效的呢？下面几个 <code>workload</code> 就能说明这个问题：</p>
<ol>
<li>全文搜索，在用户给定的内容中寻找内容，或者给出一定答案。此处的回答一定是和用户内容强相关的，本质上会复读一部分；</li>
<li>代码生成场景，变量名、函数名等都很容易被 <code>ngram</code> 预测。</li>
</ol>
<h4 id="Tree-Verification"><a href="#Tree-Verification" class="headerlink" title="Tree Verification"></a>Tree Verification</h4><p><img src="/2025/04/28/vllm/tree_verification.png"></p>
<h4 id="Model-based（draft-model）Speculative-Decoding"><a href="#Model-based（draft-model）Speculative-Decoding" class="headerlink" title="Model-based（draft model）Speculative Decoding"></a>Model-based（draft model）Speculative Decoding</h4><ol>
<li><p>Parallel guessing（并行猜测）</p>
<ul>
<li>优点：快，在不知道第一个 token 情况下直接猜第二个。</li>
<li>缺点：在猜测第二个 token 的时候不知道第一个token是什么，容易胡言乱语</li>
</ul>
</li>
<li><p>Autoregression guessing（自回归猜测）</p>
<ul>
<li>优点：在猜测第二个 token 的时候知道第一个 token，准确率较高</li>
<li>缺点：慢</li>
</ul>
</li>
</ol>

    </div>
    
    
    
    
    <div id="comment">
        <div id="giscus-container" class="giscus"></div>
    </div>
    
    
    
    
</div>

            <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2022 - 2025 Okabe&#39;s LAB
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;Zihong Lin
        </div>
        <div>
            Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp;
            <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a>
        </div>
        
    </div>
</footer>

        </div>
        
        <transition name="fade">
            <div id="preview" ref="preview" v-show="previewShow">
                <img id="preview-content" ref="previewContent" />
            </div>
        </transition>
        
    </div>
    <script src="/js/main.js"></script>
    
    
<script
    src="https://giscus.app/client.js"
    data-repo="Okabe-Rintarou-0/Okabe-Rintarou-0.github.io"
    data-repo-id="R_kgDOJD3YPA"
    data-category="Announcements"
    data-category-id="DIC_kwDOJD3YPM4Cc9W2"
    data-mapping="pathname"
    data-strict="0"
    data-reactions-enabled="1"
    data-emit-metadata="0"
    data-input-position="bottom"
    data-theme="preferred_color_scheme"
    data-lang="zh-CN"
    crossorigin
    async
></script>





    
</body>
</html>
