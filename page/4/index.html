
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <title>Okabe&#39;s LAB</title>
    <meta name="author" content="Zihong Lin" />
    <meta name="description" content="" />
    <meta name="keywords" content="" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <link rel="icon" href="/images/avatar.jpg" />
    <link rel="preconnect" href="https://cdn.staticfile.org" />
<script src="https://cdn.staticfile.org/vue/3.3.7/vue.global.prod.min.js"></script>
<link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/6.4.2/css/all.min.css" />
<link rel="preconnect" href="https://fonts.loli.net" />
<link rel="preconnect" href="https://gstatic.loli.net" crossorigin />
<link rel="stylesheet" href="https://fonts.loli.net/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap" />
<script> const mixins = {}; </script>

<script src="https://polyfill.io/v3/polyfill.min.js?features=default"></script>


<script src="https://cdn.staticfile.org/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.org/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script>
<link
    rel="stylesheet"
    href="https://cdn.staticfile.org/highlight.js/11.9.0/styles/github.min.css"
/>
<script src="/js/lib/highlight.js"></script>



<script src="/js/lib/preview.js"></script>





<script src="/js/lib/home.js"></script>

<link rel="stylesheet" href="/css/main.css" />

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.1.1"></head>
<body>
    <div id="layout">
        <transition name="fade">
            <div id="loading" v-show="loading">
                <div id="loading-circle">
                    <h2>LOADING</h2>
                    <p>加载过慢请开启缓存 浏览器默认开启</p>
                    <img src="/images/loading.gif" />
                </div>
            </div>
        </transition>
        <div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}">
    <nav id="desktop-menu">
        <a class="title" href="/">
            <span>OKABE&#39;S LAB</span>
        </a>
        
        <a href="/">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;Home</span>
        </a>
        
        <a href="/about">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;About</span>
        </a>
        
        <a href="/archives">
            <i class="fa-solid fa-box-archive fa-fw"></i>
            <span>&ensp;Archives</span>
        </a>
        
        <a href="/categories">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;Categories</span>
        </a>
        
        <a href="/tags">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;Tags</span>
        </a>
        
    </nav>
    <nav id="mobile-menu">
        <div class="title" @click="showMenuItems = !showMenuItems">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;OKABE&#39;S LAB</span>
        </div>
        <transition name="slide">
            <div class="items" v-show="showMenuItems">
                
                <a href="/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-house fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Home</div>
                    </div>
                </a>
                
                <a href="/about">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-id-card fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">About</div>
                    </div>
                </a>
                
                <a href="/archives">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-box-archive fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Archives</div>
                    </div>
                </a>
                
                <a href="/categories">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-bookmark fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Categories</div>
                    </div>
                </a>
                
                <a href="/tags">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-tags fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Tags</div>
                    </div>
                </a>
                
            </div>
        </transition>
    </nav>
</div>
<transition name="fade">
    <div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div>
</transition>

        <div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'">
            <div id="home-head">
    <div id="home-background" ref="homeBackground" data-images="/images/background.jpg"></div>
    <div id="home-info" @click="homeClick">
        <span class="loop"></span>
        <span class="loop"></span>
        <span class="loop"></span>
        <span class="loop"></span>
        <span class="info">
            <div class="wrap">
                <h1>Okabe&#39;s LAB</h1>
                <h3></h3>
                <h5></h5>
            </div>
        </span>
    </div>
</div>
<div id="home-posts-wrap" true ref="homePostsWrap">
    <div id="home-posts">
        

<div class="post">
    <a href="/2022/12/03/diffusion/">
        <h2 class="post-title">Diffusion Model 笔记</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/AI/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                AI
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2022/12/3
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <ul>
<li><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1re4y1m7gb/?spm_id_from=333.999.0.0&vd_source=f7e4c2acec163bdcd3e200e3623cc3e3">简单易懂diffusion模型讲解 - 从前置知识深度生成模型 隐变量 VAE开始</a></p>
<p>  up 主：<a target="_blank" rel="noopener" href="https://space.bilibili.com/10407305">是好梦梦哦</a></p>
<p>  对应的 PPT：<a href="./doc/%E7%AE%80%E5%8D%95%E6%98%93%E6%87%82diffusion%E6%A8%A1%E5%9E%8B%E8%AE%B2%E8%A7%A3%20-%20%E4%BB%8E%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%20%E9%9A%90%E5%8F%98%E9%87%8F%20VAE%E5%BC%80%E5%A7%8B.pptx">简单易懂diffusion模型讲解 - 从前置知识深度生成模型 隐变量 VAE开始</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1ne411u7J6/?spm_id_from=333.999.0.0&vd_source=f7e4c2acec163bdcd3e200e3623cc3e3">非常详尽的数学推理</a></p>
</li>
</ul>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/AI/" style="color: #00bcd4">AI</a>
        </span>
        
        <span class="tag">
            
            <a href="/tags/Diffusion-Model/" style="color: #03a9f4">Diffusion Model</a>
        </span>
        
    </div>
    <a href="/2022/12/03/diffusion/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2022/12/02/auto-encoder/">
        <h2 class="post-title">Auto Encoder 笔记</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/AI/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                AI
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2022/12/2
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <p><img src="/2022/12/02/auto-encoder/ae4.png"></p>
<h2 id="Image"><a href="#Image" class="headerlink" title="Image"></a>Image</h2><p>训练一个 <code>Encoder</code> 和 <code>Decoder</code>：</p>
<ul>
<li><code>Encoder</code> 将图像这样高维度的信息压缩为一个低维度的向量（Dimension Reduction）。</li>
<li><code>Decoder</code> 将这个低维度的向量重新复原为原始图像。</li>
</ul>
<p><img src="/2022/12/02/auto-encoder/ae1.png"></p>
<h3 id="为什么这种方法行之有效？"><a href="#为什么这种方法行之有效？" class="headerlink" title="为什么这种方法行之有效？"></a>为什么这种方法行之有效？</h3><p>为什么我们能够将一个高维度的图像信息压缩为一个很低维度的向量？因为对于图像而言，绝大多数都是无意义的噪声，<br>就像你随机生成一个灰度图的二维矩阵，大概率得到的是一堆噪音。而真正有意义的图像其实只占极少的比例。</p>
<p>以 3x3 的图像举例，在这么小的图像里，可能只有两种布局是有意义的，那么 <code>Encoder</code> 完全可以输出一个二维的向量，<br>代表这两种情况。</p>
<p><img src="/2022/12/02/auto-encoder/ae2.png"></p>
<p>也就是说，在图像这样的极高维空间中，真正有意义的图像只占很小的空间，我们可以用压缩的信息（类似于 Word Embedding）将它们尽可能表示出来 。</p>
<h2 id="De-noising-Auto-encoder"><a href="#De-noising-Auto-encoder" class="headerlink" title="De-noising Auto-encoder"></a>De-noising Auto-encoder</h2><p><code>BERT</code> 的预训练也用到了去噪自编码器的思想。</p>
<p><img src="/2022/12/02/auto-encoder/ae3.png"></p>
<h2 id="Feature-Disentangle"><a href="#Feature-Disentangle" class="headerlink" title="Feature Disentangle"></a>Feature Disentangle</h2><p><img src="/2022/12/02/auto-encoder/ae5.png"></p>
<p>Encoder 虽然能够将多种特征，信息提取成一个向量，但是我们无法知道向量中哪个维度对应了什么。<code>Feature Disentangle</code> 就是要去解决这个问题。</p>
<p><img src="/2022/12/02/auto-encoder/ae6.png"></p>
<h3 id="应用：Voice-Conversion"><a href="#应用：Voice-Conversion" class="headerlink" title="应用：Voice Conversion"></a>应用：Voice Conversion</h3><p>既然我们可以用 <code>Feature Disentangle</code> 来了解哪些维度对应了声音的特征，哪些维度对应了声音的内容，<br>那我们就可以选择对应声音的特征的维度进行声音的替换（保持声音内容的维度不变，将声音特征的部分替换成新垣结衣的）。</p>
<p><img src="/2022/12/02/auto-encoder/ae7.png"></p>
<h2 id="Discrete-Latent-Representation"><a href="#Discrete-Latent-Representation" class="headerlink" title="Discrete Latent Representation"></a>Discrete Latent Representation</h2><p><img src="/2022/12/02/auto-encoder/ae8.png"></p>
<h2 id="Text-as-Representation"><a href="#Text-as-Representation" class="headerlink" title="Text as Representation"></a>Text as Representation</h2><p>尝试让 <code>Encoder</code> 生成摘要（既然 <code>Decoder</code> 能够将其还原，说明 <code>Encoder</code> 生成的东西代表了文章的核心和精华内容）。 </p>
<p>和 <code>CycleGAN</code> 一样。如果不使用 <code>Discriminator</code> 会导致这个摘要完全不可读（<code>Encoder</code> 和 <code>Decoder</code> 在加密通话）</p>
<h2 id="Generator-what-VAE-does"><a href="#Generator-what-VAE-does" class="headerlink" title="Generator(what VAE does)"></a>Generator(what VAE does)</h2><p><img src="/2022/12/02/auto-encoder/ae9.png"></p>
<h2 id="Compression"><a href="#Compression" class="headerlink" title="Compression"></a>Compression</h2><p><img src="/2022/12/02/auto-encoder/ae10.png"></p>
<h2 id="Anomaly-Detection（异常检测）"><a href="#Anomaly-Detection（异常检测）" class="headerlink" title="Anomaly Detection（异常检测）"></a>Anomaly Detection（异常检测）</h2><p>异常检测是一个 <code>One-class</code> 的分类问题，数据集是极度不对称的：你有大量正常的数据，和极少数不正常的数据，甚至你不知道不正常的数据长什么样，<br>这时候你要怎么训练？这不是一个一般的分类问题，需要用 <code>Auto-encoder</code> 来进行解决。</p>
<p>判断是否能够重构：</p>
<p><img src="/2022/12/02/auto-encoder/ae11.png"></p>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/AI/" style="color: #03a9f4">AI</a>
        </span>
        
        <span class="tag">
            
            <a href="/tags/Anto-Encoder/" style="color: #00bcd4">Anto Encoder</a>
        </span>
        
    </div>
    <a href="/2022/12/02/auto-encoder/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2022/12/01/bert/">
        <h2 class="post-title">BERT 笔记</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/AI/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                AI
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2022/12/1
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805v2">https://arxiv.org/abs/1810.04805v2</a></p>
<p><img src="/2022/12/01/bert/BERT0.jpg"></p>
<blockquote>
<p>BERT是2018年10月由Google AI研究院提出的一种预训练模型。BERT的全称是Bidirectional Encoder Representation from Transformers。BERT在机器阅读理解顶级水平测试SQuAD1.1中表现出惊人的成绩: 全部两个衡量指标上全面超越人类，并且在11种不同NLP测试中创出SOTA表现，包括将GLUE基准推高至80.4% (绝对改进7.6%)，MultiNLI准确度达到86.7% (绝对改进5.6%)，成为NLP发展史上的里程碑式的模型成就。</p>
</blockquote>
<p>芝麻街大家族。</p>
<p><img src="/2022/12/01/bert/BERT1.png"></p>
<h2 id="BERT-的结构"><a href="#BERT-的结构" class="headerlink" title="BERT 的结构"></a>BERT 的结构</h2><p>沿用了 <code>Transformer</code> 的 <code>Encoder</code>。</p>
<p><img src="/2022/12/01/bert/BERT14.jpg"></p>
<p><img src="/2022/12/01/bert/BERT15.jpg"></p>
<h2 id="BERT-是怎么学做填空题的？"><a href="#BERT-是怎么学做填空题的？" class="headerlink" title="BERT 是怎么学做填空题的？"></a>BERT 是怎么学做填空题的？</h2><blockquote>
<p>BERT 的作者认为：使用两个方向（从左至右和从右至左）的单向编码器拼接而成的双向编码器，在性能、参数规模和效率等方面都不如直接使用双向编码器强大；这是 BERT 模型使用 Transformer Encoder 结构作为特征提取器，而不拼接使用两个方向的 Transformer Decoder 结构作为特征提取器的原因。</p>
</blockquote>
<blockquote>
<p>这也令 BERT 模型不能像 GPT 模型一样，继续使用标准语言模型的训练模式，因此 BERT 模型重新定义了两种模型训练方法（即：预训练任务）：MLM 和 NSP。BERT用MLM（Masked Language Model，掩码语言模型）方法训练词的语义理解能力，用NSP（Next Sentence Prediction，下句预测）方法训练句子之间的理解能力，从而更好地支持下游任务。</p>
</blockquote>
<h3 id="Masking-Input"><a href="#Masking-Input" class="headerlink" title="Masking Input"></a>Masking Input</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a></p>
<p>在给 <code>BERT</code> 数据的时候，将若干字替换成一个特殊的 <code>token</code>，或者随机的值。两种方式随机选择一种。</p>
<p><img src="/2022/12/01/bert/BERT3.png"></p>
<p>前面提到过，<code>BERT</code> 即是 <code>Encoder</code>，会输出一个向量的序列。 将这个向量经过一个线性层 + <code>softmax</code> 就可以得到遮住的字的概率分布。<br>我们既然已经知道“台”后面的字是“湾”，也就是说我们有标签，自然就可以用交叉熵作为损失函数训练咯~</p>
<p>这也就是“自己出题，自己回答”。</p>
<h3 id="Next-Sentence-Prediction"><a href="#Next-Sentence-Prediction" class="headerlink" title="Next Sentence Prediction"></a>Next Sentence Prediction</h3><p><code>SEP</code> 区分两个句子。</p>
<p>这边只看 <code>CLS</code> 对应的输出，经过线性变换，做一个二元分类问题（Yes&#x2F;No），即判断两个句子是不是相邻的。</p>
<h2 id="BERT-学到了什么？"><a href="#BERT-学到了什么？" class="headerlink" title="BERT 学到了什么？"></a>BERT 学到了什么？</h2><p>自监督学习。</p>
<p><img src="/2022/12/01/bert/BERT2.jpg"></p>
<p><code>BERT</code> 实际上学到了如何做填空题。</p>
<p><img src="/2022/12/01/bert/BERT5.png"></p>
<p>先预训练，然后在微调以适应不同的任务。就像一个干细胞，可以分化成任何人体细胞。</p>
<p><code>BERT</code> 本身是无监督的（自己出填空题，自己学着做），但是其下游任务（downstream task）可能是监督学习的。比如语句情感分类，利用之前学习填空题 pretrain<br>过的 <code>BERT</code> 模型，将其输出经过另一个网络（这个网络是需要投喂数据集监督学习的），最终输出情感分类。</p>
<p>pretrain 过的 <code>BERT</code> 会比随机参数的好。All you need is <strong>fine-tuning!</strong></p>
<p><img src="/2022/12/01/bert/BERT7.png"></p>
<p>对于词性识别，流程也是类似的。生成的每个向量都通过一个待训练网络，剩下的和上面的例子一样。</p>
<p><img src="/2022/12/01/bert/BERT8.jpg"></p>
<p>判断假设和推断是否矛盾：</p>
<p><img src="/2022/12/01/bert/BERT9.jpg"></p>
<p>输入文章和问题，返回两个整数（答案的起止位置）：</p>
<p><img src="/2022/12/01/bert/BERT10.jpg"></p>
<p>和自注意力机制类似，橙色部分类似于一个 <code>Query</code>，去查询 <code>document</code> 中的每一个 <code>Key</code>，最终经过 <code>softmax</code> 的到最大概率的作为 start。<br>这边 s &#x3D; 2。</p>
<p><img src="/2022/12/01/bert/BERT11.jpg"></p>
<p>还有一个蓝色的向量需要学习，对应于 end。和 start 类似，这边求出 e &#x3D; 3。于是我们得出了答案的范围。</p>
<p><img src="/2022/12/01/bert/BERT12.jpg"></p>
<p>注意橙色和蓝色向量都是要训练的对象。</p>
<h3 id="Training-BERT-is-challenging"><a href="#Training-BERT-is-challenging" class="headerlink" title="Training BERT is challenging"></a>Training BERT is challenging</h3><p>训练 <code>BERT</code> 需要极大的数据量和极强的硬件。</p>
<h3 id="预训练-Seq2Seq"><a href="#预训练-Seq2Seq" class="headerlink" title="预训练 Seq2Seq"></a>预训练 Seq2Seq</h3><p>类似的，也可以预训练 <code>Decoder</code>。将一些句子破坏掉（原始句子作为标签），进行 <code>self-supervise</code></p>
<p><img src="/2022/12/01/bert/BERT13.jpg"></p>
<h2 id="为什么-BERT-能够工作？"><a href="#为什么-BERT-能够工作？" class="headerlink" title="为什么 BERT 能够工作？"></a>为什么 BERT 能够工作？</h2><p><img src="/2022/12/01/bert/BERT18.jpg"></p>
<p>类似于 <code>word2vec</code>，<code>BERT</code> 能够将每个词都对应到一个向量（表征某个词的含义（meaning））。向量两两之间的距离代表了两个词的意思的接近程度。</p>
<p><img src="/2022/12/01/bert/BERT16.jpg"></p>
<p>但是不同的是，<code>BERT</code> 在预训练的时候会学习到上下文信息，同一个字在不同的上下文中，其含义不同，<code>BERT</code> 输出的向量也不同。所以 <code>BERT</code> 输出的<br><code>Embedding</code> 是动态的，会根据上下文的不同产生变化。</p>
<p>以 <code>苹果</code> 为例，“吃苹果”的“苹果”是一种水果，而“苹果手机”的“苹果”则是一个公司品牌名。</p>
<p>通过计算这两个苹果经过 <code>BERT</code> 输出的 <code>Embedding</code> 的 <code>cosine similarity</code>，可以得到下图：</p>
<p><img src="/2022/12/01/bert/BERT17.jpg"></p>
<h2 id="令人震惊的-BERT"><a href="#令人震惊的-BERT" class="headerlink" title="令人震惊的 BERT"></a>令人震惊的 BERT</h2><h3 id="DNA-classification"><a href="#DNA-classification" class="headerlink" title="DNA classification"></a>DNA classification</h3><p>用语言资料预训练的 <code>BERT</code> 甚至可以提升 <code>DNA</code> 分类的准确度！</p>
<p><img src="/2022/12/01/bert/BERT19.jpg"></p>
<p>因为预训练用的是英语，所以这边需要将碱基对对应到一个随机词汇上，然后再对 <code>DNA</code> 分类问题进行 <code>Fine-Tune</code>。即使是这样八竿子打不着的映射，<br>居然也能学出更好的效果——难道 <code>DNA</code> 序列结构和某种语言语法结构有关？ </p>
<p><img src="/2022/12/01/bert/BERT20.jpg"></p>
<h3 id="Multi-lingual-BERT"><a href="#Multi-lingual-BERT" class="headerlink" title="Multi-lingual BERT"></a>Multi-lingual BERT</h3><p>用多种语言的资料训练的 <code>BERT</code> 能够学习到语言之间的联系，在没有学习中文问答资料的情况下，输入中文问题居然也可以达到惊人的准确率。</p>
<p><img src="/2022/12/01/bert/BERT22.jpg"></p>
<p>似乎 <code>BERT</code> 在学到 <code>Rabbit</code> 和 <code>兔</code> 的向量距离很近的同时，也没有忘记语言类型信息。也就是在回答中文问题的时候，它不会将 <code>兔</code> 替换成 <code>Rabbit</code>，<br>而是都用中文来回答。</p>
<p><img src="/2022/12/01/bert/BERT23.jpg"></p>
<p>或许是因为，两个词汇虽然很接近，但是在它们所处的极高维空间内，它们根本不在同一个维度，只是距离相近，仅此而已。</p>
<p>如果将不同语言的所有词汇的 <code>Embedding</code> 求均值，将会发现这些语言在高维空间中散落在不同的区域。假如说你求出 <code>中文</code> 和 <code>English</code> 的这种距离，<br>然后将一个英文 <code>Embedding</code> 加上这个距离，你将会得到中文的 <code>Embedding</code>！</p>
<p><img src="/2022/12/01/bert/BERT24.jpg"></p>
<h1 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h1><p><img src="/GPT2.jpg"></p>
<h2 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h2><p>结构类似 <code>Transformer</code> 的 <code>Decoder</code>（使用 <code>Masked self-attention</code>）。</p>
<p><img src="/GPT1.jpg"></p>
<p><code>GPT</code> 和 <code>BERT</code> 不一样的地方在于 <code>GPT</code> 是在是太大了，以至于我们难以 <code>Fine-tune</code> 它。 </p>
<h3 id="In-context-learning"><a href="#In-context-learning" class="headerlink" title="In-context learning"></a>In-context learning</h3><p>这里不会去利用梯度下降更新 <code>GPT</code> 的参数。</p>
<h3 id="Few-shot-One-shot-Zero-shot-Learning"><a href="#Few-shot-One-shot-Zero-shot-Learning" class="headerlink" title="Few-shot&#x2F;One-shot&#x2F;Zero-shot Learning"></a>Few-shot&#x2F;One-shot&#x2F;Zero-shot Learning</h3><p>提供任务描述和例子，学会举一反三。</p>
<ul>
<li><p>Few-shot:</p>
<p>  <img src="/GPT3.jpg"></p>
</li>
<li><p>One-shot:</p>
<p>  <img src="/GPT4.jpg"></p>
</li>
<li><p>Zero-shot:</p>
<p>  <img src="/GPT5.jpg"></p>
</li>
</ul>
<h2 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h2><p><img src="/GPT6.jpg"></p>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/BERT/" style="color: #ffa2c4">BERT</a>
        </span>
        
        <span class="tag">
            
            <a href="/tags/Transformer/" style="color: #03a9f4">Transformer</a>
        </span>
        
        <span class="tag">
            
            <a href="/tags/NLP/" style="color: #00bcd4">NLP</a>
        </span>
        
    </div>
    <a href="/2022/12/01/bert/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2022/11/23/gan/">
        <h2 class="post-title">GAN 笔记</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/AI/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                AI
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2022/11/23
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <p>物竞天择，万物一同进化，一同内卷。<code>Generator</code> 和 <code>Discriminate</code> 一同训练和进化，此乃 <code>GAN</code> 的核心。</p>
<p><img src="/2022/11/23/gan/GAN5.png"></p>
<h2 id="为什么我们需要输出一个分布（distribution）"><a href="#为什么我们需要输出一个分布（distribution）" class="headerlink" title="为什么我们需要输出一个分布（distribution）"></a>为什么我们需要输出一个分布（distribution）</h2><p><img src="/2022/11/23/gan/GAN1.png"></p>
<p>对于吃豆人这样的游戏，假如说我们要用一帧游戏画面去预测下一帧游戏画面，如果我们使用最传统的网络，接受一个确定的输入，然后输出一个<br>确定的结果，那么可能会出现如下的情况：</p>
<p><img src="/2022/11/23/gan/GAN2.gif"></p>
<p>这是因为对于训练集中的不同数据，可能存在一帧画面完全相同，但是下一帧的画面相反（一个怪物向左，一个怪物向右）。模型为了获得最低的损失，<br>会倾向于输出既向左又向右的叠加态。</p>
<p>本质上这是因为我们输出的是一个固定的值&#x2F;矩阵，而不是一个分布。假设我们输出的是一个分布，那么我们可以在输入的时候也加上一个分布 <code>z</code>（见第一张图），<br>来对应到输出的分布<code>y</code>。这样我们就可以用不同的输入分布来对应各种可能的情况：怪物向左，向右，向上，向下，死亡，存活。</p>
<p>本质上，这样的 <code>Generator</code> 适用于需要创造力的场景。</p>
<h2 id="Unconditional-Generation"><a href="#Unconditional-Generation" class="headerlink" title="Unconditional Generation"></a>Unconditional Generation</h2><p><img src="/2022/11/23/gan/GAN3.png"></p>
<p>这里输入的分布可以是已知的，简单的分布，比如 <code>Normal Distribution</code>，网络会想办法生成一个复杂的分布。这个分布也就是一个高维的向量，<br>经过整理之后就会变成输出的图像。</p>
<p>我们还需要一个 <code>Discriminator</code>。它会接受 <code>Generator</code> 输出的图片作为输入，并且输出一个数值表示该图片是否是真实的二次元图片。</p>
<p><img src="/2022/11/23/gan/GAN4.png"></p>
<h3 id="训练步骤"><a href="#训练步骤" class="headerlink" title="训练步骤"></a>训练步骤</h3><ol>
<li>初始化 <code>Generator</code> 和 <code>Discriminator</code></li>
<li>固定 <code>Generator</code> 的网络参数，训练 <code>Discriminator</code>，让它学会 <strong>“打假”</strong>：<br> <img src="/2022/11/23/gan/GAN6.png"><ol>
<li>根据输入分布随机采样一些向量，经过 <code>Generator</code> 得到一些生成图片，这些图像是 <strong>虚假的二次元图像</strong> 标记为 0</li>
<li>从二次元图像数据集中采样一些图片，作为 <strong>真实的二次元图片</strong>，标记为 1</li>
<li>根据这些打好标签的图片，训练 <code>Discriminator</code>，本质上就是一个分类问题，训练一个 <code>classifier</code>。</li>
</ol>
</li>
<li>固定 <code>Discriminator</code> 的网络参数，训练 <code>Generator</code>，让它学会 <strong>“造假”</strong>：<br> <img src="/2022/11/23/gan/GAN7.png"><br> 将 <code>Generator</code> 的网络和 <code>Discriminator</code> 的网络拼接，但是固定后者的参数。两者之间会有一个 <code>hidden layer</code>，将其整理就是生成的图片。<ol>
<li>根据输入分布生成向量，传入 <code>Generator</code>，生成图片，由 <code>Discriminator</code> 生成一个分数（目标是这个分数越大越好）。</li>
<li>根据 <code>Discriminator</code> 的打分，训练 <code>Generator</code>。</li>
</ol>
</li>
</ol>
<h2 id="GAN-的学习目标是什么？"><a href="#GAN-的学习目标是什么？" class="headerlink" title="GAN 的学习目标是什么？"></a>GAN 的学习目标是什么？</h2><p>我们希望 <code>GAN</code> 生成的分布 <code>PG</code>和真实的数据分布 <code>Pdata</code> 越接近越好（divergence 最小，即两种分布之间的某种距离）</p>
<p><img src="/2022/11/23/gan/GAN8.png"></p>
<h3 id="如何计算-Divergence？"><a href="#如何计算-Divergence？" class="headerlink" title="如何计算 Divergence？"></a>如何计算 <code>Divergence</code>？</h3><p><img src="/2022/11/23/gan/GAN9.png"></p>
<blockquote>
<p>Sampling is good enough.</p>
</blockquote>
<ul>
<li>从数据集 sample 真实的图像，即对 <code>Pdata</code> 的 <strong>sampling</strong></li>
<li>根据输入的分布 sample 出一些向量，通过 <code>Generator</code> 得到图像，即对 <code>PG</code> 的 <strong>sampling</strong></li>
</ul>
<p>在只知道 sample 的情况如何估算 <code>divergence</code>？</p>
<p>下图是对于 <code>Discriminator</code> 的训练目标，蓝色星星代表真实数据的分布，橙色的星星代表生出数据的分布。我们的目标是让 <code>Discriminator</code> 能够区分两种分布。</p>
<p><img src="/2022/11/23/gan/GAN10.png"></p>
<p>目标函数的左半部分代表我们希望由 <code>Discriminator</code> 生成的真实数据的期望分数应该越高越好，而生成数据的分数应该越低越好。损失函数可以是另外的形式，<br>这里是为了和二分类问题扯上关系。目标函数其实就是交叉熵乘上符号，也就是说我们要最大化目标函数，等价于最小化交叉熵。这与二分类问题的损失函数定义是类似的。</p>
<p>也就是我们在训练一个 <code>Classifier</code>。</p>
<p><img src="/2022/11/23/gan/GAN11.png"></p>
<h2 id="JS-divergence-不合适？"><a href="#JS-divergence-不合适？" class="headerlink" title="JS divergence 不合适？"></a>JS divergence 不合适？</h2><p>对于高维空间的两个分布（想象二维情况下两个直线），重叠部分几乎可以忽略不计。即使有重叠，如果采样数量不够，在 <code>Discriminator</code> 看来，两个分布依旧没有重叠。</p>
<p>JS divergence 不合适的理由在于，对于两个无重叠的分布，JS divergence会变为一个常数：log2，这将导致梯度消失，无法从 JS divergence 中获取两个分布的距离信息。<br>因为无论训练出来的分布和真实分布有多近，如果两者没有重叠的话，JS 散度永远是常数，也就无法由此来更新网络参数了。</p>
<p>（有关 JS divergence 可以看这篇文章：<a target="_blank" rel="noopener" href="https://blog.csdn.net/Invokar/article/details/88917214">GAN：两者分布不重合JS散度为log2的数学证明</a> ）</p>
<p><img src="/2022/11/23/gan/GAN13.png"></p>
<h3 id="Wasserstein-distance"><a href="#Wasserstein-distance" class="headerlink" title="Wasserstein distance"></a>Wasserstein distance</h3><p>可以通俗的认为将一个分布变为另一个分布所需要的代价。分布就像是一堆堆土堆，然后用挖土机将一堆土堆变为另一堆土堆的模样的消耗，即为 <code>Wasserstein distance</code>。这本身也是一个优化问题，<br>穷举所有的 <code>moving plan</code>，看哪一个可以获得最短的平均距离，这个最短平均距离作为 <code>Wasserstein distance</code>。</p>
<p><img src="/2022/11/23/gan/GAN14.png"></p>
<p>解决了 JS divergence 的缺陷：</p>
<p><img src="/2022/11/23/gan/GAN15.png"></p>
<h2 id="WGAN"><a href="#WGAN" class="headerlink" title="WGAN"></a>WGAN</h2><p><img src="/2022/11/23/gan/GAN16.png"></p>
<p>计算 <code>Wasserstein distance</code>，D 必须是平滑的函数，避免值剧烈变化，导致生成的分布和真实分布轻微偏离就产生无穷大的值。 </p>
<p><code>WGAN</code> 这篇论文其实也没有找这样的函数（比较困难），而是将 <code>Wasserstein distance</code> 限制在 [-c, c] 之间（c 为常数）。 </p>
<p><img src="/2022/11/23/gan/GAN17.png"></p>
<h2 id="GAN-存在的困难之处"><a href="#GAN-存在的困难之处" class="headerlink" title="GAN 存在的困难之处"></a>GAN 存在的困难之处</h2><ul>
<li><p>难以训练：</p>
<p>  GAN 是训练两个模型，一旦一个模型的训练出了问题，另一个模型也会出问题。也就是两个人一直在卷，卷到一半对手开摆，你也开摆！</p>
<p>  各种训练的 tips：</p>
<ul>
<li><p>Tips from Soumith</p>
<p>  <a target="_blank" rel="noopener" href="https://github.com/soumith/ganhacks">https://github.com/soumith/ganhacks</a></p>
</li>
<li><p>Tips in DCGAN: Guideline for network architecture design for image generation </p>
<p>  <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1511.06434">https://arxiv.org/abs/1511.06434</a></p>
</li>
<li><p>Improved techniques for training GANs </p>
<p>  <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1606.03498">https://arxiv.org/abs/1606.03498</a></p>
</li>
<li><p>Tips from BigGAN</p>
<p>  <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.11096">https://arxiv.org/abs/1809.11096</a></p>
</li>
</ul>
</li>
<li><p>难以做序列生成：</p>
<p>  <img src="/2022/11/23/gan/GAN18.png"></p>
<p>  Generator 的参数变化会导致输出向量的微小变化，但是由于输出的是一个概率分布，概率分布的微小变化不会显著影响取到最大值的那一维度，也就是微小变化可能不会导致输出词汇的变化。<br>  导致 Discriminator 无法进一步学习（梯度消失！）。</p>
</li>
<li><p>Mode Collapse</p>
<p>  <img src="/2022/11/23/gan/GAN19.png"></p>
<p>  Generator 发现生成某一个特征的图片可以永远骗过 Discriminator，于是它就倾向于一直输出类似的图片。</p>
</li>
<li><p>Mode Dropping </p>
<p>  <img src="/2022/11/23/gan/GAN20.png"></p>
<p>  多样性的丧失。</p>
</li>
</ul>
<h2 id="GAN-评估"><a href="#GAN-评估" class="headerlink" title="GAN 评估"></a>GAN 评估</h2><ul>
<li><p>Inception score</p>
<p>  <img src="/2022/11/23/gan/GAN21.png"></p>
<p>  问题在于，对于hw，判定图片质量的方式只是检测识别到了多少张人脸，而非检测红发，黑发，蓝发等多样性特征。</p>
</li>
<li><p>FID</p>
<p>  <img src="/2022/11/23/gan/GAN22.png"></p>
<p>  取出 hidden layer 的向量而非输出的类别向量，计算分布的 <code>FID</code></p>
</li>
</ul>
<h2 id="Conditional-Generator"><a href="#Conditional-Generator" class="headerlink" title="Conditional Generator"></a>Conditional Generator</h2><p>我们无法使用之前无条件生成的架构，因为 Discriminator 只是在学习如何打假，而没有学习生成的图片是否满足给定的条件。</p>
<p><img src="/2022/11/23/gan/GAN23.png"></p>
<p>需要如下图所示的成对训练资料：</p>
<p><img src="/2022/11/23/gan/GAN24.png"></p>
<p>不同于之前无条件的情况进行“打假”，将所有训练集的数据都标注为真，生成的都标注为假，这里还需要选中一些训练集的数据（不满足给定条件），标注他们为假。</p>
<h2 id="CycleGAN：GAN-与无监督学习"><a href="#CycleGAN：GAN-与无监督学习" class="headerlink" title="CycleGAN：GAN 与无监督学习"></a>CycleGAN：GAN 与无监督学习</h2><p><img src="/2022/11/23/gan/GAN25.png"></p>
<p>对于这种风格变换的类型，我们很难找到成对的训练资料。但是我们可以用 GAN 处理这种类型的问题。</p>
<p><img src="/2022/11/23/gan/GAN26.png"></p>
<p>和原先 GAN 训练方式类似，之前是从高斯分布中 sample 一个向量出来，现在则是从 <code>Domain X</code> 中 sample 一张图片。然后将这张图片输入 Generator，<br>然后获得生成的风格变换的图片。这些图片和真实的图片作为训练集来训练 Discriminator，和之前的训练方式也是类似的。</p>
<p>但是这样可能会产生模型忽略输入而产生不相关图片的现象（反正输出二次元图片就能高分，那我干嘛要管输入的三次元图像）。</p>
<p>CycleGAN 通过两个 Generator 解决了这样的问题：一个 Generator 生成目标图像（即风格变换后的图像），另一个负责将这个目标图像变回原图。</p>
<p><img src="/2022/11/23/gan/GAN27.png"></p>
<p>通俗来讲，就是第一个 Generator 负责将三次元人物变成二次元纸片人；而第二个 Generator 负责将这个生成的二次元纸片人恢复成原来的三次元人物。</p>
<p>这样就构成了循环，也就是 CycleGAN 名字的由来。这样的循环的好处在于，假如说第一个 Generator 无视输入的原图的特征，而生成一些毫不相关的二次元图像，<br>那么第二个 Generator 就很难将其转变回原来的图像（因为丢失了很多原图的语义信息）。这样就会产生较大的 loss，对这种行为进行惩罚。</p>
<p>虽然说 Gx-&gt;y 和 Gy-&gt;x可能串通一气，前者和后者都将图片镜像翻转，这样后者依旧能恢复出原始输入图像。但是一般实际训练的时候，第一个 Generator 就会生成和原图比较像的图片了（不会做复杂的变换），<br>所以 CycleGAN 在实际应用中很少会发生上述的情况。</p>
<p>你也可以训练一个双向的 CycleGAN：</p>
<p><img src="/2022/11/23/gan/GAN28.png"></p>
<p><img src="/2022/11/23/gan/GAN29.jpg"></p>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/AI/" style="color: #ff7d73">AI</a>
        </span>
        
        <span class="tag">
            
            <a href="/tags/GAN/" style="color: #00a596">GAN</a>
        </span>
        
    </div>
    <a href="/2022/11/23/gan/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2022/11/20/self-attention/">
        <h2 class="post-title">Self Attention 笔记</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/AI/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                AI
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2022/11/20
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <ul>
<li><p>自注意力机制的矩阵运算：</p>
<p> <code>q</code> 即 <code>query</code>，<code>k</code> 即 <code>key</code>，<code>v</code> 即 <code>value</code></p>
<p>  <img src="/2022/11/20/self-attention/self-attention1.png"></p>
<p>  <code>α</code> 即为注意力分数，经过 <code>softmax</code> 进行归一化得到 <code>α&#39;</code>，并与 <code>v</code> 相乘得到最终的 <code>b</code></p>
<p>  <img src="/2022/11/20/self-attention/self-attention2.png"></p>
<p>  <img src="/2022/11/20/self-attention/self-attention3.png"></p>
</li>
<li><p>为什么我们需要多头注意力机制（<code>Multi-head Self-attention</code>）？</p>
<p>  <code>Self-attention</code> 实际上就是去找和 <code>q</code> 相关的 <code>k</code>，但是相关性是一个比较复杂的东西，我们往往需要多个维度对相关性进行描述，<br>  即使用多个 <code>q</code> 负责不同的相关性。</p>
<p>  <img src="/2022/11/20/self-attention/self-attention4.png"></p>
<p>  将多个头获取的 <code>bi</code> 进行 <strong>concat</strong>：</p>
<p>  <img src="/2022/11/20/self-attention/self-attention5.png"></p>
</li>
<li><p>有什么不足？缺少位置信息：</p>
<p>  解决方案：<code>positional vector</code>，每一个位置都对应一个不同的位置向量 <code>ei</code>，</p>
<p>  <img src="/2022/11/20/self-attention/self-attention6.png"></p>
</li>
<li><p>我们真的需要<strong>读一整句话</strong>吗？</p>
<p>  对于像语音识别这样的序列而言，我们会把每 10ms 的语音数据作为一个向量，那么一句话就会对应相当可观的序列长度 <code>L</code>。而我们产生的注意力矩阵的规模和 <code>L</code> 的二次成正比，<br>  如果 <code>L</code> 非常大的话，注意力矩阵也会相当大。所以我们实际上不会读一整个句子，而是使用所谓 <code>Truncated Self-attention</code> 去读一个窗口（其大小可以自行指定）</p>
<p>  <img src="/2022/11/20/self-attention/self-attention7.png"></p>
</li>
<li><p><code>Self-attention</code> vs <code>CNN</code>：</p>
<p>  图片亦可作为 <code>set of vector</code>，所以 <code>Self-attention</code> 亦可用于图像领域。<code>CNN</code> 的卷积核代表了一片感受野，其大小是受限的，而 <code>Self-attention</code> 则关注了整张图片的信息。</p>
<blockquote>
<p>CNN is simplified self-attention(self-attention that only attends in a receptive field).</p>
</blockquote>
<p>  或者你也可以说：</p>
<blockquote>
<p>Self-attention is the complex version of CNN(CNN with learnable receptive field).</p>
</blockquote>
<p>  <img src="/2022/11/20/self-attention/self-attention8.png"></p>
</li>
<li><p><code>Self-attention</code> vs <code>RNN</code>：</p>
<p>  <code>RNN</code> 只能获取左边已输入的序列的信息吗？不，可以用双向 <code>RNN</code>，即 <code>Bidirectional RNN</code></p>
<p>  <code>Self-attention</code> 可以并行处理（矩阵运算，GPU 优化），并且可以轻易获取非常远的上下文信息。</p>
<p>  <img src="/2022/11/20/self-attention/self-attention9.png"></p>
</li>
<li><p>在图中使用自注意力机制：</p>
<p>  <img src="/2022/11/20/self-attention/self-attention10.png"></p>
</li>
</ul>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/AI/" style="color: #ffa2c4">AI</a>
        </span>
        
        <span class="tag">
            
            <a href="/tags/Self-Attention/" style="color: #03a9f4">Self Attention</a>
        </span>
        
    </div>
    <a href="/2022/11/20/self-attention/" class="go-post">阅读全文</a>
</div>

<div class="post">
    <a href="/2022/11/20/transformer/">
        <h2 class="post-title">Transformer 笔记——Attention Is All You Need</h2>
    </a>
    <div class="category-and-date">
        
        <span class="category">
            <a href="/categories/AI/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                AI
            </a>
        </span>
        
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2022/11/20
        </span>
        
        
    </div>
    <div class="description">
        <div class="content" v-pre>
            
            <h2 id="Core——注意力计算公式"><a href="#Core——注意力计算公式" class="headerlink" title="Core——注意力计算公式"></a>Core——注意力计算公式</h2><p><img src="/2022/11/20/transformer/attention.png"></p>
<h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><p><img src="/2022/11/20/transformer/transformer1.png"></p>
<h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><ul>
<li><p>Add &amp; Norm</p>
<p>  利用了 <code>Residual</code> 的思想，于此同时加上一个 <code>Layer Normalization</code>。</p>
<p>  （关于 <code>Layer Norm</code> 和 <code>Batch Norm</code> 的区别，请看 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/74516930">NLP中 batch normalization与 layer normalization</a> ）</p>
<p>  <img src="/2022/11/20/transformer/transformer2.png"></p>
<p>  多头注意力模块 + <code>Add &amp; Norm</code> 构成了图中深蓝色的模块，这个模块将会再被用于图中最右的结构。</p>
</li>
</ul>
<h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p><code>Decoder</code> 分为两种：<code>AT</code> 和 <code>NAT</code></p>
<ul>
<li><p>AT</p>
<p>  接受 <code>Encoder</code> 的输出，以及自己之前的输出作为输入，反复生成下一个字。</p>
<p>  以语音识别为例，输出是一段中文，需要有一个特殊 token 代表开始和结束。</p>
<p>  <img src="/2022/11/20/transformer/transformer3.png"></p>
</li>
<li><p>NAT</p>
<p>  不同于 <code>AT</code> 会一直迭代生成直到输出一个 <code>END</code>，<code>NAT</code> 是给若干个 <code>START</code> 然后并行生成若干个字符。</p>
<p>  <img src="/2022/11/20/transformer/transformer4.png"></p>
</li>
</ul>
<h2 id="Encoder-和-Decoder-结构对比"><a href="#Encoder-和-Decoder-结构对比" class="headerlink" title="Encoder 和 Decoder 结构对比"></a>Encoder 和 Decoder 结构对比</h2><ul>
<li><p>Decoder 的多头注意力机制模块是 <code>masked</code> 的：</p>
<p>  未被掩码的 <code>Self-attention</code> 会观察整个上下文，但是对于 <code>decoder</code> 而言不能观察后文（因为生成一个字的时候，后面的字还未生成，只能获取前文的上下文信息）。<br>  比如已经生成“机器”的时候，即将生成“学”，模型只能从“机器”这一前文观察到上下文信息，因为此时“习”还没有被生成。</p>
<p>  本质上就是输入到 <code>encoder</code> 的序列是已知的，可以一窥全文，而 <code>decoder</code> 则是循环生成序列，还未生成，何来后文？</p>
</li>
</ul>
<h2 id="Cross-Attention"><a href="#Cross-Attention" class="headerlink" title="Cross Attention"></a>Cross Attention</h2><p>将 <code>decoder</code> 的向量的 <code>q</code> 和 <code>encoder</code> 的向量的 <code>k</code> 和 <code>v</code> 共同计算 <code>attention score</code>，即为 <code>Cross Attention</code>。</p>
<p><img src="/2022/11/20/transformer/transformer5.png"></p>
<h2 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h2><ul>
<li><p>Teacher Forcing</p>
<p>  在训练 <code>decoder</code> 的时候避免 <code>Error propogation</code>（一步错步步错），投喂 <code>Ground truth</code>。</p>
<p>  <code>decoder</code> 输出的是一个概率分布，因而使用 <code>cross entropy</code> 作为损失函数。</p>
<p>  <img src="/2022/11/20/transformer/transformer6.png"></p>
<p>  存在的问题：训练的时候“偷看”了正确结果，但是测试的时候怎么办呢？存在 mismatch。</p>
</li>
<li><p>Tips</p>
<ul>
<li><p>Copy Mechanism</p>
<p>  应对难以生成或者无需凭空生成（比如生成摘要）序列的情况：</p>
<p>  <img src="/2022/11/20/transformer/transformer7.png"></p>
</li>
<li><p>Guided Attention</p>
<p>  对 Attention 的训练加以限制，比如语音识别的 Attention 必须从左到右：</p>
<p>  <img src="/2022/11/20/transformer/transformer8.png"></p>
</li>
<li><p>Beam Search</p>
<p>  是对 <code>Greedy Search</code> 的一个改进算法。相对 <code>Greedy Search</code> 扩大了搜索空间，但远远不及穷举搜索指数级的搜索空间，是二者的一个折中方案。</p>
<p>  <img src="/2022/11/20/transformer/transformer9.png"></p>
</li>
<li><p>对于语音生成，在测试的时候添加噪音反而会有好的结果：可能是语音的 <code>ground truth</code> 不是一个绝对的东西，比如”你好“可能对应男生，女生，中性等多种音调的声音。</p>
</li>
<li><p>无法 optimize 的函数（不可微分） 可以考虑用 RL 硬 train？</p>
</li>
<li><p>上面提到的 mismatch 问题可以用 <code>Schedule Sampling</code> 解决：在 <code>Teacher Forcing</code> 的时候投喂一些错误的 <code>ground truth</code>，比如<code>机气学习</code></p>
</li>
</ul>
</li>
</ul>

            
        </div>
    </div>
    <div class="post-tags">
        
        <span class="icon">
            <i class="fa-solid fa-tags fa-fw"></i>
        </span>
        
        
        
        <span class="tag">
            
            <a href="/tags/AI/" style="color: #00a596">AI</a>
        </span>
        
        <span class="tag">
            
            <a href="/tags/Transformer/" style="color: #03a9f4">Transformer</a>
        </span>
        
    </div>
    <a href="/2022/11/20/transformer/" class="go-post">阅读全文</a>
</div>


        <div class="page-current">
    
    <a class="page-num" href="/page/3/">
        <i class="fa-solid fa-caret-left fa-fw"></i>
    </a>
    <a class="page-num" href="/">1</a>
    <span class="page-omit">...</span>
    
    <span class="current">4</span>
    
    
    
    
</div>

    </div>
    
    <div id="home-card">
        <div id="card-style">
    <div id="card-div">
        <div class="avatar">
            <img src="/images/avatar.jpg" alt="avatar" />
        </div>
        <div class="name">Zihong Lin</div>
        <div class="description">
            <p>What I can’t create, I don’t understand.</p>

        </div>
        
        
    </div>
</div>

    </div>
    
</div>

            <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2022 - 2025 Okabe&#39;s LAB
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;Zihong Lin
        </div>
        <div>
            Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp;
            <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a>
        </div>
        
    </div>
</footer>

        </div>
        
        <transition name="fade">
            <div id="preview" ref="preview" v-show="previewShow">
                <img id="preview-content" ref="previewContent" />
            </div>
        </transition>
        
    </div>
    <script src="/js/main.js"></script>
    
</body>
</html>
